{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:49:40.699321Z",
     "start_time": "2023-11-05T13:49:40.686364800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "df = pd.read_csv('resume_data.csv')\n",
    "df.drop_duplicates(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:49:41.327271600Z",
     "start_time": "2023-11-05T13:49:41.119631Z"
    }
   },
   "id": "4ffbeb13cabc5ed0"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))  # Используйте соответствующий язык\n",
    "stemmer = SnowballStemmer('russian')  # Используйте соответствующий языкэ\n",
    "def preprocess_text(text):\n",
    "    # Приведем к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удалим пунктуацию\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    # Токенизация\n",
    "    words = word_tokenize(text)\n",
    "    # Удалим стоп-слова и применим стемминг\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    \n",
    "    return words\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:49:42.115303600Z",
     "start_time": "2023-11-05T13:49:42.094080100Z"
    }
   },
   "id": "48f702afd5e09080"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "[(4370, 0.1822537109446224),\n (4427, 0.1822537109446224),\n (359, 0.1814582571844196),\n (1185, 0.167091878148945),\n (1613, 0.167091878148945),\n (1339, 0.1517321718494116),\n (1066, 0.14168032429384336),\n (2203, 0.14168032429384336),\n (0, 0.13863365099652675),\n (878, 0.13713678945565017),\n (1278, 0.13402344034895164),\n (1020, 0.12925062976427612),\n (1350, 0.12925062976427612),\n (1470, 0.12925062976427612),\n (391, 0.12370081513315381),\n (4472, 0.12324036697863898),\n (3013, 0.11929722697167877),\n (4046, 0.11929722697167877),\n (4651, 0.11867890721217506),\n (1299, 0.11780961371568188),\n (268, 0.11523513511783762),\n (303, 0.11523513511783762),\n (496, 0.11492231068025222),\n (651, 0.11492231068025222),\n (4678, 0.1113336879520386),\n (971, 0.10999517572794403),\n (490, 0.10848760469709087),\n (646, 0.10848760469709087),\n (2439, 0.10711932894755084),\n (3241, 0.10711932894755084),\n (386, 0.10442479515870423),\n (3592, 0.1028139094938835),\n (2276, 0.10124658173830012),\n (3527, 0.09958830772651157),\n (411, 0.0993334898711646),\n (579, 0.0993334898711646),\n (770, 0.0993334898711646),\n (3747, 0.09905623088275377),\n (4673, 0.09688994099442355),\n (1009, 0.09625691663720144),\n (1457, 0.09625691663720144),\n (512, 0.09467548035658485),\n (673, 0.09467548035658485),\n (695, 0.09467548035658485),\n (1273, 0.09463450739973005),\n (779, 0.09351318755761268),\n (69, 0.09206561556236639),\n (112, 0.09206561556236639),\n (2414, 0.09131499552995437),\n (3423, 0.09131499552995437),\n (3829, 0.08997154349560599),\n (4743, 0.08917501241558551),\n (51, 0.08900298092470592),\n (3462, 0.08869148825434363),\n (4593, 0.08869148825434363),\n (1564, 0.08390528535159716),\n (2334, 0.08241827964068514),\n (3170, 0.08241827964068514),\n (4302, 0.08241827964068514),\n (4718, 0.08205073396778002),\n (358, 0.08158773603889662),\n (4459, 0.08101480950531509),\n (1106, 0.0797843816771623),\n (1683, 0.0797843816771623),\n (2513, 0.07859905346049757),\n (3349, 0.07859905346049757),\n (2978, 0.07859549768944785),\n (3952, 0.07859549768944785),\n (1118, 0.07853351595078961),\n (1705, 0.07853351595078961),\n (1762, 0.07853351595078961),\n (425, 0.0777124644776164),\n (215, 0.07702743423144306),\n (4092, 0.07438794180668598),\n (59, 0.07310032074130263),\n (100, 0.07310032074130263),\n (1291, 0.07272215861839001),\n (1427, 0.07258517827941055),\n (1542, 0.07258517827941055),\n (1647, 0.07236371675946501),\n (2215, 0.07236371675946501),\n (2502, 0.07196083042630662),\n (3332, 0.07196083042630662),\n (3486, 0.0710687277478663),\n (4625, 0.0710687277478663),\n (368, 0.07105299902500568),\n (2354, 0.07060180783561146),\n (4349, 0.07060180783561146),\n (4404, 0.07060180783561146),\n (3214, 0.06971150252129794),\n (279, 0.06964180336305634),\n (318, 0.06964180336305634),\n (1137, 0.06880962504086797),\n (1735, 0.06880962504086797),\n (1786, 0.06880962504086797),\n (572, 0.06869167525631725),\n (762, 0.06869167525631725),\n (375, 0.06830617698701151),\n (3457, 0.06818605518887665),\n (4587, 0.06818605518887665),\n (1463, 0.0680212471471129),\n (180, 0.06792167129000846),\n (1283, 0.0678127717880166),\n (2437, 0.06767989770782136),\n (3238, 0.06767989770782136),\n (527, 0.06762528630646955),\n (708, 0.06762528630646955),\n (155, 0.06706051756233858),\n (1300, 0.06681770545136423),\n (10, 0.06676149820437466),\n (1574, 0.06614207063994526),\n (4465, 0.06596660972941382),\n (4462, 0.06577776661061448),\n (1807, 0.065472039261697),\n (754, 0.06534030476013897),\n (540, 0.06500249959351147),\n (728, 0.06500249959351147),\n (3881, 0.06407396667044474),\n (160, 0.06356551435510863),\n (4754, 0.06344556199923745),\n (4775, 0.06312237521600775),\n (3139, 0.0627948731384432),\n (4249, 0.0627948731384432),\n (612, 0.06265749925528073),\n (2751, 0.06241640262772123),\n (3571, 0.06241640262772123),\n (798, 0.06233280895414316),\n (2746, 0.06200667310373946),\n (3563, 0.06200667310373946),\n (2338, 0.061655908722411654),\n (4311, 0.061655908722411654),\n (151, 0.06156812305444707),\n (3847, 0.06146252517610527),\n (792, 0.06123124591658065),\n (90, 0.061217601679045386),\n (144, 0.061217601679045386),\n (259, 0.061204915329607604),\n (4799, 0.06115172524369505),\n (2790, 0.05981095298161564),\n (4519, 0.0598033870781766),\n (1141, 0.059791798667362776),\n (1789, 0.059791798667362776),\n (1965, 0.059416778654725576),\n (2598, 0.059416778654725576),\n (2676, 0.059416778654725576),\n (1115, 0.05934045951022964),\n (1697, 0.05934045951022964),\n (1745, 0.05934045951022964),\n (4828, 0.05924990808138922),\n (2731, 0.05901386982441179),\n (3540, 0.05901386982441179),\n (2509, 0.05890706754579619),\n (3339, 0.05890706754579619),\n (922, 0.05870023418985375),\n (2714, 0.05865514760127479),\n (3528, 0.05865514760127479),\n (2380, 0.058626548420368144),\n (4441, 0.058626548420368144),\n (2051, 0.05847331928709854),\n (2294, 0.05837099274807959),\n (2035, 0.05812266767184712),\n (3452, 0.05741086666850651),\n (4579, 0.05741086666850651),\n (989, 0.05731934238019745),\n (2087, 0.057243678735562034),\n (2151, 0.057243678735562034),\n (1035, 0.05684516373698138),\n (1379, 0.05684516373698138),\n (1494, 0.05684516373698138),\n (4793, 0.056115073229866086),\n (1897, 0.05578441240934674),\n (2540, 0.05578441240934674),\n (2620, 0.05578441240934674),\n (335, 0.0556502850975579),\n (902, 0.055244185376803916),\n (1039, 0.05512076268639412),\n (1382, 0.05512076268639412),\n (1497, 0.05512076268639412),\n (3519, 0.054765975217914445),\n (4147, 0.054298063713469576),\n (2516, 0.054284348851615476),\n (3348, 0.054284348851615476),\n (4013, 0.0541891921148847),\n (2251, 0.05399941433029415),\n (2123, 0.053980119405029794),\n (2189, 0.053980119405029794),\n (2897, 0.05381725914819057),\n (2476, 0.05375455430116953),\n (3283, 0.05375455430116953),\n (382, 0.053730886246661974),\n (193, 0.05342220491803308),\n (203, 0.05342220491803308),\n (1911, 0.05266828321426331),\n (2557, 0.05266828321426331),\n (2638, 0.05266828321426331),\n (421, 0.0517978119070852),\n (3026, 0.051793489701484174),\n (4068, 0.051793489701484174),\n (4213, 0.051345372943711144),\n (3713, 0.051279360896092925),\n (3741, 0.05121951464767798),\n (3145, 0.05116136602633565),\n (4255, 0.05116136602633565),\n (4277, 0.05116136602633565),\n (1744, 0.05097245413680279),\n (657, 0.050852293408214344),\n (688, 0.050852293408214344),\n (2894, 0.050758353582878765),\n (3294, 0.050746599973736546),\n (3299, 0.050746599973736546),\n (604, 0.050606524864110936),\n (1138, 0.0505076379764627),\n (1790, 0.0505076379764627),\n (4051, 0.05048205491075116),\n (1375, 0.05025687097446941),\n (1488, 0.05025687097446941),\n (2118, 0.0502359384337861),\n (2190, 0.0502359384337861),\n (2870, 0.050168889723554766),\n (1058, 0.050160191494762904),\n (2125, 0.050160191494762904),\n (2192, 0.050160191494762904),\n (810, 0.050056126832633954),\n (3615, 0.0498892964943983),\n (2272, 0.049859928915279274),\n (2386, 0.04925901502809081),\n (4447, 0.04925901502809081),\n (1813, 0.04918375179726826),\n (1922, 0.04910900308923763),\n (1939, 0.04910900308923763),\n (2569, 0.04910900308923763),\n (2649, 0.04910900308923763),\n (1640, 0.048957376201959876),\n (2207, 0.048957376201959876),\n (1804, 0.048816851918147816),\n (3772, 0.04857142923963951),\n (2030, 0.0482653168045778),\n (2821, 0.0482653168045778),\n (4803, 0.048251852815772975),\n (56, 0.04823024433229823),\n (240, 0.0481676668730194),\n (1087, 0.048155526044791286),\n (1662, 0.048155526044791286),\n (2349, 0.047908504504974664),\n (3200, 0.047908504504974664),\n (4341, 0.047908504504974664),\n (4394, 0.047908504504974664),\n (1822, 0.047648519246844837),\n (4686, 0.04763967585663739),\n (4568, 0.047582486156480724),\n (350, 0.0475446159258569),\n (2075, 0.047450549402480224),\n (2144, 0.047450549402480224),\n (3968, 0.04716436182665022),\n (3987, 0.04716436182665022),\n (1853, 0.046734095933968815),\n (2416, 0.04668080040383814),\n (3426, 0.04668080040383814),\n (1812, 0.04652235696062385),\n (775, 0.0463897587785921),\n (3420, 0.0462665800915252),\n (4027, 0.04612005340700923),\n (3289, 0.046036607820974004),\n (68, 0.045338366998555334),\n (111, 0.045338366998555334),\n (3733, 0.045180170896603686),\n (1419, 0.045179488105012496),\n (1535, 0.045179488105012496),\n (3320, 0.04508519649085496),\n (597, 0.04500055806227919),\n (747, 0.044969953144506684),\n (4000, 0.044932891322943425),\n (1805, 0.04479958980288437),\n (2794, 0.044381407059921135),\n (3475, 0.044244474913945175),\n (4612, 0.044244474913945175),\n (473, 0.04415251087953119),\n (910, 0.04415251087953119),\n (31, 0.04407716200975434),\n (3613, 0.04395576348160354),\n (4170, 0.043828831263745906),\n (2764, 0.043677898244579655),\n (3583, 0.043677898244579655),\n (2878, 0.043597193843714285),\n (3137, 0.04356053396238563),\n (4246, 0.04356053396238563),\n (355, 0.04354208991563963),\n (2494, 0.04353404103802492),\n (3318, 0.04353404103802492),\n (4674, 0.043532951328520954),\n (3607, 0.04348403091948451),\n (3154, 0.04347685378235441),\n (4269, 0.04347685378235441),\n (4287, 0.04347685378235441),\n (8, 0.04347559904287444),\n (1189, 0.04338363214678675),\n (1617, 0.04338363214678675),\n (1025, 0.04328808763692644),\n (1362, 0.04328808763692644),\n (1476, 0.04328808763692644),\n (4099, 0.04324250875277575),\n (547, 0.043217872294919475),\n (734, 0.043217872294919475),\n (2224, 0.04306603713536514),\n (4725, 0.043017044227796765),\n (3167, 0.04300903841945395),\n (4305, 0.04300903841945395),\n (2830, 0.04300692297568402),\n (2122, 0.04298872873128786),\n (2191, 0.04298872873128786),\n (4763, 0.04288463702670882),\n (1928, 0.042871966352270655),\n (1943, 0.042871966352270655),\n (2573, 0.042871966352270655),\n (2654, 0.042871966352270655),\n (4482, 0.04283557172130202),\n (3661, 0.04273123258570358),\n (2697, 0.042567492666677656),\n (435, 0.042260606236748995),\n (786, 0.042238889497335466),\n (982, 0.04222947615032559),\n (1352, 0.04222947615032559),\n (637, 0.04222910194359623),\n (1828, 0.04209339862957011),\n (1143, 0.042055530741796535),\n (430, 0.041898650630730915),\n (1088, 0.04184691187961344),\n (1660, 0.04184691187961344),\n (248, 0.04183246106445733),\n (1438, 0.04175730305364576),\n (979, 0.041746503097140654),\n (1348, 0.041746503097140654),\n (2783, 0.041563618749067686),\n (2348, 0.04155544481231721),\n (4339, 0.04155544481231721),\n (4398, 0.04155544481231721),\n (238, 0.04154562652464999),\n (2907, 0.0415289323348797),\n (967, 0.04151753218028573),\n (1334, 0.04151753218028573),\n (4648, 0.04139936285969353),\n (1060, 0.041297853898439735),\n (2196, 0.041297853898439735),\n (851, 0.04113559355039507),\n (1892, 0.041045887015907584),\n (2537, 0.041045887015907584),\n (3295, 0.04100844702225008),\n (3303, 0.04100844702225008),\n (3697, 0.04091199517024813),\n (4239, 0.04083217372578197),\n (2796, 0.04073083053286094),\n (2732, 0.04056254042937868),\n (3545, 0.04056254042937868),\n (4751, 0.04049404632389655),\n (541, 0.040470715287963074),\n (725, 0.040470715287963074),\n (3707, 0.040420051954548696),\n (1319, 0.040419705870567035),\n (3644, 0.04038634729891706),\n (401, 0.040259312954118144),\n (563, 0.040259312954118144),\n (753, 0.040259312954118144),\n (2310, 0.04019871249115752),\n (1999, 0.039985095627418296),\n (209, 0.039975830987281046),\n (2440, 0.03987048820602938),\n (3242, 0.03987048820602938),\n (2887, 0.03979648533042783),\n (844, 0.039776639551608796),\n (4764, 0.039701266260123394),\n (1027, 0.039653717685022676),\n (1366, 0.039653717685022676),\n (1480, 0.039653717685022676),\n (3456, 0.03964996861968118),\n (4588, 0.03964996861968118),\n (2912, 0.039446786801963306),\n (1550, 0.03943038315623662),\n (1208, 0.0394229648928618),\n (2237, 0.03940564577133492),\n (3489, 0.0393795871261355),\n (4626, 0.0393795871261355),\n (4165, 0.039358950529351455),\n (4130, 0.0392261358729854),\n (784, 0.039165440485576825),\n (2025, 0.03916222197182628),\n (2814, 0.03916222197182628),\n (419, 0.03905958427662933),\n (588, 0.03905958427662933),\n (448, 0.038978829510816354),\n (874, 0.038978829510816354),\n (82, 0.03895316640389114),\n (130, 0.03895316640389114),\n (332, 0.038943881704530635),\n (3188, 0.03892944898838353),\n (4326, 0.03892944898838353),\n (4381, 0.03892944898838353),\n (4738, 0.03886996034370584),\n (3614, 0.03885423899556638),\n (605, 0.038826710217769386),\n (4816, 0.03879433004732308),\n (2481, 0.03863391307134994),\n (3292, 0.03863391307134994),\n (3300, 0.03863391307134994),\n (2016, 0.038574988091222145),\n (595, 0.03846026435586745),\n (3058, 0.03843541007224456),\n (3778, 0.03823901865974851),\n (1184, 0.038204020238648155),\n (1607, 0.038204020238648155),\n (1915, 0.03818533504258223),\n (1934, 0.03818533504258223),\n (2564, 0.03818533504258223),\n (2640, 0.03818533504258223),\n (3431, 0.03810854089634645),\n (3700, 0.03809655793029599),\n (1874, 0.03791826165794994),\n (1854, 0.03785693928332502),\n (4122, 0.03783815466600314),\n (278, 0.03781983000270381),\n (317, 0.03781983000270381),\n (2390, 0.037748888903303524),\n (4458, 0.037748888903303524),\n (1019, 0.03765387701490092),\n (1356, 0.03765387701490092),\n (1471, 0.03765387701490092),\n (778, 0.037595305007342525),\n (1154, 0.037532749880562245),\n (1571, 0.037532749880562245),\n (1708, 0.037514286091089234),\n (1756, 0.037514286091089234),\n (1213, 0.03732736611675663),\n (2968, 0.037222204542547405),\n (3938, 0.037222204542547405),\n (3024, 0.03719739072160762),\n (4061, 0.03719739072160762),\n (244, 0.037195323443997015),\n (2892, 0.03707823336464122),\n (373, 0.03701361315840251),\n (1002, 0.03683101954550774),\n (2406, 0.03681553242412472),\n (3402, 0.03681553242412472),\n (2322, 0.036768989504105856),\n (1158, 0.036766192691474464),\n (1577, 0.036766192691474464),\n (3366, 0.03675696905666697),\n (1821, 0.036754744939253695),\n (1598, 0.036657882678865106),\n (531, 0.036502650068232015),\n (714, 0.036502650068232015),\n (3814, 0.036404561927679525),\n (432, 0.03637468381328163),\n (1363, 0.03631825942022632),\n (1479, 0.03631825942022632),\n (1075, 0.03628888447842833),\n (2211, 0.03628888447842833),\n (2511, 0.03623547531796158),\n (3344, 0.03623547531796158),\n (342, 0.03622663608326239),\n (4256, 0.036085405053464956),\n (4276, 0.036085405053464956),\n (1406, 0.0360336664964331),\n (1526, 0.0360336664964331),\n (4157, 0.03601637437444574),\n (1932, 0.03592720525948878),\n (1947, 0.03592720525948878),\n (2581, 0.03592720525948878),\n (2660, 0.03592720525948878),\n (481, 0.0357516591289911),\n (940, 0.0357516591289911),\n (4116, 0.03572433962734747),\n (550, 0.03569562280291508),\n (740, 0.03569562280291508),\n (441, 0.03569016448370248),\n (3388, 0.035640966403816095),\n (1952, 0.03541968269928981),\n (2583, 0.03541968269928981),\n (2662, 0.03541968269928981),\n (2905, 0.03540623705171902),\n (4015, 0.03534937004527816),\n (1844, 0.03530566137287181),\n (2008, 0.035301788824309),\n (75, 0.03525731798076892),\n (123, 0.03525731798076892),\n (2446, 0.03525340848951665),\n (3253, 0.03525340848951665),\n (3028, 0.03521525539186695),\n (4070, 0.03521525539186695),\n (1445, 0.03519760631903445),\n (3886, 0.03518216532803062),\n (1973, 0.03516141182595857),\n (2612, 0.03516141182595857),\n (2691, 0.03516141182595857),\n (3746, 0.03508111154611164),\n (70, 0.03506624208037476),\n (114, 0.03506624208037476),\n (21, 0.03504416926149852),\n (1219, 0.03502568869188034),\n (491, 0.03502491584014435),\n (647, 0.03502491584014435),\n (2939, 0.03497192231613687),\n (3900, 0.03497192231613687),\n (993, 0.0349423926214692),\n (4719, 0.03483513929852489),\n (619, 0.03482057897497236),\n (1094, 0.03480075456138623),\n (1668, 0.03480075456138623),\n (2103, 0.034755558646178346),\n (2173, 0.034755558646178346),\n (1236, 0.03475408408670505),\n (1163, 0.03472737202439829),\n (1588, 0.03472737202439829),\n (2782, 0.03471492830124561),\n (4760, 0.03466241232770518),\n (3343, 0.03462083522561347),\n (2497, 0.034537837285255814),\n (3321, 0.034537837285255814),\n (3196, 0.03425479132376439),\n (4334, 0.03425479132376439),\n (4390, 0.03425479132376439),\n (3453, 0.03421054930542004),\n (4584, 0.03421054930542004),\n (2086, 0.03417944875886654),\n (2152, 0.03417944875886654),\n (2097, 0.03415313103395981),\n (2165, 0.03415313103395981),\n (283, 0.034079332705167345),\n (316, 0.034079332705167345),\n (2449, 0.03400778322878329),\n (3256, 0.03400778322878329),\n (4810, 0.03396538229885234),\n (965, 0.03395595138717096),\n (1330, 0.03395595138717096),\n (2332, 0.033873731770276576),\n (3164, 0.033873731770276576),\n (4301, 0.033873731770276576),\n (3667, 0.03386606819918911),\n (1377, 0.03382215559319875),\n (1491, 0.03382215559319875),\n (1919, 0.03380151839557292),\n (1937, 0.03380151839557292),\n (2568, 0.03380151839557292),\n (2645, 0.03380151839557292),\n (3877, 0.033799658102845215),\n (3752, 0.033737664960188186),\n (3769, 0.033663860416170456),\n (2759, 0.033651774023737736),\n (3577, 0.033651774023737736),\n (3646, 0.03361017034320481),\n (3076, 0.033600495603954526),\n (3622, 0.033595159029682936),\n (1104, 0.03354296405508414),\n (1684, 0.03354296405508414),\n (4512, 0.03348845672845098),\n (1261, 0.033471784824569625),\n (1433, 0.03345252539133626),\n (3492, 0.033449400312648085),\n (4630, 0.033449400312648085),\n (3585, 0.033397416759611304),\n (585, 0.03335049319114173),\n (463, 0.0331504895010839),\n (897, 0.0331504895010839),\n (2942, 0.033128690715957546),\n (3904, 0.033128690715957546),\n (1268, 0.03309657630363894),\n (510, 0.03303818433280307),\n (662, 0.03303818433280307),\n (379, 0.03290434985302322),\n (3520, 0.03289950244970471),\n (3710, 0.032880188903606666),\n (4528, 0.03285751462110343),\n (1170, 0.032837541252835),\n (1594, 0.032837541252835),\n (4144, 0.032783766637763646),\n (1307, 0.032747691409943505),\n (2065, 0.03271594167681355),\n (2134, 0.03271594167681355),\n (4343, 0.032709410134596245),\n (4396, 0.032709410134596245),\n (380, 0.03263851179119743),\n (4667, 0.032637642929919976),\n (1112, 0.03256165167147686),\n (1691, 0.03256165167147686),\n (4705, 0.032549081225062754),\n (1245, 0.03254460789765491),\n (2235, 0.03232206717919347),\n (3842, 0.03231857962437462),\n (177, 0.03228460631244352),\n (469, 0.03225242787596076),\n (907, 0.03225242787596076),\n (189, 0.03216894238424849),\n (201, 0.03216894238424849),\n (3788, 0.032165938488565936),\n (1169, 0.03214260550066509),\n (1597, 0.03214260550066509),\n (2923, 0.03204686550844799),\n (586, 0.03199919283860667),\n (287, 0.03191737528473999),\n (323, 0.03191737528473999),\n (640, 0.03191466811872842),\n (3743, 0.03188707734715371),\n (1806, 0.03181518316693958),\n (1070, 0.03176272950300097),\n (2204, 0.03176272950300097),\n (1063, 0.03176113725890029),\n (2199, 0.03176113725890029),\n (868, 0.03175124231156685),\n (2270, 0.031699499842131496),\n (3110, 0.031609434894711824),\n (4204, 0.031609434894711824),\n (1, 0.031591238471224956),\n (966, 0.031589364061515746),\n (1328, 0.031589364061515746),\n (1424, 0.031584787258380015),\n (1541, 0.031584787258380015),\n (4139, 0.031540073813760124),\n (1870, 0.03148763148024459),\n (1159, 0.0314302810296912),\n (1578, 0.0314302810296912),\n (4843, 0.031417746897310535),\n (1370, 0.03136173686757893),\n (1486, 0.03136173686757893),\n (973, 0.03134371762782632),\n (1338, 0.03134371762782632),\n (3837, 0.031152716468932466),\n (983, 0.031080386644342137),\n (1349, 0.031080386644342137),\n (3929, 0.031067360117900955),\n (2784, 0.031054741392458098),\n (3775, 0.03102885268604379),\n (4748, 0.03100496382019012),\n (841, 0.031003762883951184),\n (2098, 0.030839219458429743),\n (2167, 0.030839219458429743),\n (4823, 0.030818205748416556),\n (1279, 0.030803373765931737),\n (3856, 0.030786626038304003),\n (2388, 0.0307364499449845),\n (4454, 0.0307364499449845),\n (3379, 0.03071851807455006),\n (4848, 0.03065071069292855),\n (2874, 0.03064390574427495),\n (4504, 0.0305972980434328),\n (3390, 0.030567747595411435),\n (1325, 0.030450109284471793),\n (3491, 0.030448862088486295),\n (4629, 0.030448862088486295),\n (3500, 0.030403874413510674),\n (1551, 0.030377244628086414),\n (630, 0.03035843701601504),\n (437, 0.030248835195687048),\n (2394, 0.030246069189120785),\n (1130, 0.030226758701659653),\n (1723, 0.030226758701659653),\n (1773, 0.030226758701659653),\n (2040, 0.030210237448820593),\n (1196, 0.030153913445735248),\n (1624, 0.030153913445735248),\n (3461, 0.029994573977287026),\n (4591, 0.029994573977287026),\n (4440, 0.029990622214428102),\n (2061, 0.02996964341910575),\n (2133, 0.02996964341910575),\n (3673, 0.029936306133416343),\n (3438, 0.029838820143120094),\n (4565, 0.029838820143120094),\n (1074, 0.029819810526632788),\n (2208, 0.029819810526632788),\n (2316, 0.029805368680550375),\n (3784, 0.029789662984429345),\n (3346, 0.029776397001245302),\n (2404, 0.02976977959175398),\n (1732, 0.029726997133080784),\n (1782, 0.029726997133080784),\n (3946, 0.029640160874315483),\n (1089, 0.029564910185132402),\n (1663, 0.029564910185132402),\n (972, 0.029562514552801518),\n (1336, 0.029562514552801518),\n (2379, 0.029540941347789545),\n (4437, 0.029540941347789545),\n (241, 0.02952820562470294),\n (1042, 0.02952742387685781),\n (1385, 0.02952742387685781),\n (1504, 0.02952742387685781),\n (1823, 0.02952581997483808),\n (2941, 0.029504076143197194),\n (3903, 0.029504076143197194),\n (4839, 0.029494288344682776),\n (2269, 0.029479665263802066),\n (3896, 0.029477036394644007),\n (4744, 0.029464573817969016),\n (2028, 0.02944524883514612),\n (2812, 0.02944524883514612),\n (467, 0.02940737202209081),\n (899, 0.02940737202209081),\n (2896, 0.02937981093036265),\n (354, 0.029379805515091645),\n (4600, 0.02931329852516276),\n (3355, 0.02930784690393666),\n (3126, 0.02928814184267066),\n (4231, 0.02928814184267066),\n (276, 0.029286296701560813),\n (313, 0.029286296701560813),\n (107, 0.029260788217222625),\n (2043, 0.029257656995535803),\n (429, 0.02919021054499734),\n (1228, 0.02912788882035786),\n (3767, 0.029113501031008897),\n (1329, 0.029075000461275993),\n (1861, 0.029060731071398855),\n (3771, 0.029050340872230636),\n (3080, 0.029031493894586688),\n (626, 0.02895292526107196),\n (270, 0.028907543788524982),\n (304, 0.028907543788524982),\n (54, 0.028903052594210207),\n (4137, 0.02888549994376845),\n (3818, 0.02876258674625278),\n (3184, 0.028665187897384897),\n (4320, 0.028665187897384897),\n (4376, 0.028665187897384897),\n (4106, 0.02860361303324733),\n (2055, 0.02856986431153684),\n (2884, 0.02856585081551609),\n (2804, 0.028403947902817084),\n (2247, 0.02838787790331792),\n (507, 0.028382350043746087),\n (669, 0.028382350043746087),\n (3324, 0.028367696881981777),\n (61, 0.028333686116568873),\n (106, 0.028333686116568873),\n (3787, 0.02833093639299352),\n (3470, 0.028323624509346605),\n (4605, 0.028323624509346605),\n (3798, 0.028299906340748363),\n (4660, 0.028257407572740215),\n (4173, 0.0282552235662262),\n (1980, 0.028253852543759825),\n (2699, 0.028253852543759825),\n (227, 0.02816934560677878),\n (4533, 0.028108620250213108),\n (482, 0.02810520334769264),\n (935, 0.02810520334769264),\n (3212, 0.028077463415624816),\n (684, 0.028060826113435737),\n (706, 0.028060826113435737),\n (3369, 0.02805357345944396),\n (1749, 0.027984963091751844),\n (1364, 0.02789528581900374),\n (1481, 0.02789528581900374),\n (3384, 0.027846006611752853),\n (1073, 0.02782365778277096),\n (2210, 0.02782365778277096),\n (1558, 0.027717512164887378),\n (4128, 0.027690225976819072),\n (837, 0.027685326709152767),\n (1064, 0.027680410189922247),\n (2126, 0.027680410189922247),\n (4026, 0.027635457706028668),\n (2102, 0.027526201663235175),\n (2170, 0.027526201663235175),\n (4352, 0.027469608767650058),\n (4409, 0.027469608767650058),\n (4014, 0.02737418137327953),\n (1830, 0.027346688160585556),\n (1836, 0.027346688160585556),\n (3181, 0.027343334990688226),\n (4319, 0.027343334990688226),\n (4372, 0.027343334990688226),\n (4522, 0.027332356394685352),\n (176, 0.027305636231783205),\n (813, 0.027281757165978335),\n (3176, 0.027277344306839396),\n (4313, 0.027277344306839396),\n (4371, 0.027277344306839396),\n (858, 0.027266401224524693),\n (2730, 0.027265401246889618),\n (3542, 0.027265401246889618),\n (4742, 0.027244108917995124),\n (35, 0.02723884374149484),\n (864, 0.027229943634790976),\n (2965, 0.027194853811232985),\n (3942, 0.027194853811232985),\n (2460, 0.02719308836128779),\n (3269, 0.02719308836128779),\n (1186, 0.027186566615890365),\n (1614, 0.027186566615890365),\n (361, 0.027185530937752916),\n (4515, 0.02706346972596641),\n (773, 0.027056511435726086),\n (4105, 0.027031395869949616),\n (3131, 0.027030687963412386),\n (4235, 0.027030687963412386),\n (363, 0.02698785341470606),\n (2899, 0.02696176341468232),\n (3387, 0.026891084987785394),\n (4168, 0.02676230709775889),\n (3628, 0.026744878554791143),\n (2368, 0.02669900677114842),\n (4368, 0.02669900677114842),\n (4424, 0.02669900677114842),\n (3179, 0.026625866516693065),\n (4316, 0.026625866516693065),\n (4373, 0.026625866516693065),\n (623, 0.02658757520267991),\n (1891, 0.026583168430677664),\n (2531, 0.026583168430677664),\n (2069, 0.02658136462723236),\n (2136, 0.02658136462723236),\n (1071, 0.026549474821238365),\n (2206, 0.026549474821238365),\n (2315, 0.026538200357401866),\n (1977, 0.0263769467619186),\n (2694, 0.0263769467619186),\n (3744, 0.026328358214505396),\n (2998, 0.026320155188548924),\n (3997, 0.026320155188548924),\n (555, 0.026231387592539894),\n (748, 0.026231387592539894),\n (3050, 0.0262224844531508),\n (4700, 0.02621308716819988),\n (3205, 0.026193918929911908),\n (3640, 0.02616878942724106),\n (2834, 0.026155134848917676),\n (2266, 0.026093943304477042),\n (3064, 0.026014280836910007),\n (3021, 0.02601143806504115),\n (4063, 0.02601143806504115),\n (162, 0.02600059822220017),\n (2718, 0.02590472972002761),\n (3529, 0.02590472972002761),\n (2076, 0.025867442225534283),\n (2143, 0.025867442225534283),\n (170, 0.02585858051942677),\n (618, 0.02585057236637467),\n (3517, 0.025761228879015018),\n (247, 0.02575665978496587),\n (1455, 0.02572340023843501),\n (1913, 0.0257220236918402),\n (1933, 0.0257220236918402),\n (2561, 0.0257220236918402),\n (2641, 0.0257220236918402),\n (2760, 0.025700523650715512),\n (3578, 0.025700523650715512),\n (477, 0.025667517177044435),\n (923, 0.025667517177044435),\n (4750, 0.02563319737106281),\n (1988, 0.025632353538961056),\n (2704, 0.025632353538961056),\n (2370, 0.025617527645181448),\n (4369, 0.025617527645181448),\n (4425, 0.025617527645181448),\n (1444, 0.02561515679023474),\n (977, 0.0256094126193499),\n (1342, 0.0256094126193499),\n (3803, 0.025563316738527936),\n (4788, 0.02550851209927032),\n (4174, 0.02547835781293116),\n (2919, 0.025427150168025957),\n (471, 0.025418438371872114),\n (909, 0.025418438371872114),\n (1274, 0.025376074333628557),\n (1586, 0.02537087898820111),\n (1146, 0.025361055663340314),\n (1791, 0.025361055663340314),\n (1423, 0.025343190389249277),\n (1538, 0.025343190389249277),\n (814, 0.02533987339301419),\n (3599, 0.025335096935946093),\n (3751, 0.02533468621381588),\n (3443, 0.025270909371536872),\n (4569, 0.025270909371536872),\n (4671, 0.025268377630189614),\n (1210, 0.025246723158352108),\n (166, 0.02524320210562519),\n (4811, 0.02519637020556511),\n (3511, 0.025177081630610106),\n (2836, 0.025129570083805834),\n (4497, 0.025105959831905),\n (4771, 0.025103052183684468),\n (1240, 0.02508698930749184),\n (3934, 0.025083573258642293),\n (1003, 0.025062027817606546),\n (1451, 0.025062027817606546),\n (360, 0.025049627224994167),\n (3852, 0.025038992984153807),\n (2312, 0.02496139725570001),\n (876, 0.024950317487563968),\n (2886, 0.024942389312491122),\n (2288, 0.024931762392226674),\n (2972, 0.02490146864888146),\n (3943, 0.02490146864888146),\n (3512, 0.024895529602475583),\n (3813, 0.024853116021446107),\n (3761, 0.02484860582048313),\n (4167, 0.024840715017125564),\n (2095, 0.024807616021888135),\n (2166, 0.024807616021888135),\n (1893, 0.024797108014744065),\n (2534, 0.024797108014744065),\n (2734, 0.024789315506307794),\n (3546, 0.024789315506307794),\n (4081, 0.024774672521827145),\n (532, 0.024768848340862827),\n (4791, 0.024756916810094967),\n (4509, 0.02475527151300875),\n (1093, 0.024715521891545772),\n (1669, 0.024715521891545772),\n (1628, 0.0246608561222056),\n (4781, 0.024620891465585912),\n (3189, 0.024537998014462756),\n (4327, 0.024537998014462756),\n (4385, 0.024537998014462756),\n (1784, 0.024511704431452543),\n (2974, 0.024462674744822298),\n (3949, 0.024462674744822298),\n (4649, 0.024437768677480008),\n (2726, 0.024413973455860494),\n (3541, 0.024413973455860494),\n (293, 0.024406767029024355),\n (329, 0.024406767029024355),\n (4094, 0.02440088136615145),\n (4503, 0.024399303550910405),\n (2865, 0.024394572465205776),\n (1548, 0.024393097894691473),\n (3053, 0.02438576789096195),\n (4685, 0.0243755959195342),\n (1109, 0.024341059313555978),\n (1690, 0.024341059313555978),\n (1738, 0.024341059313555978),\n (848, 0.02429456437062879),\n (4254, 0.024281762794015782),\n (4822, 0.024252699574628944),\n (1816, 0.02425244519925271),\n (3789, 0.02424013756218739),\n (1984, 0.02422981119301052),\n (2702, 0.02422981119301052),\n (3219, 0.024129908862196573),\n (1097, 0.024088883153062672),\n (1672, 0.024088883153062672),\n (1322, 0.02408171449452577),\n (1650, 0.024027534022490613),\n (2221, 0.024027534022490613),\n (1080, 0.024024837379964255),\n (1643, 0.024024837379964255),\n (2216, 0.024024837379964255),\n (1800, 0.023993514860222897),\n (643, 0.023982262488803967),\n (4670, 0.023934625072522746),\n (960, 0.02390870433500869),\n (4564, 0.02389579167454644),\n (3325, 0.02388365849267933),\n (3439, 0.023872199703423514),\n (4562, 0.023872199703423514),\n (1147, 0.02383031158156714),\n (3441, 0.023818011393640755),\n (4566, 0.023818011393640755),\n (1689, 0.023789422092584927),\n (1740, 0.023789422092584927),\n (36, 0.023752012512077623),\n (498, 0.02372361356026694),\n (649, 0.02372361356026694),\n (2488, 0.023713056109595988),\n (3311, 0.023713056109595988),\n (2769, 0.02370474739148589),\n (3587, 0.02370474739148589),\n (1105, 0.02368989708416452),\n (1687, 0.02368989708416452),\n (1901, 0.02365360520103414),\n (2538, 0.02365360520103414),\n (2618, 0.02365360520103414),\n (4436, 0.023652777593204867),\n (164, 0.023641764598747134),\n (3864, 0.02361615131417002),\n (85, 0.023613675199012165),\n (135, 0.023613675199012165),\n (1297, 0.02360739896489476),\n (523, 0.023593385628955042),\n (678, 0.023593385628955042),\n (701, 0.023593385628955042),\n (1986, 0.023575097084403467),\n (2708, 0.023575097084403467),\n (4836, 0.02354448135327871),\n (2430, 0.023543217268580705),\n (3234, 0.023543217268580705),\n (2995, 0.023528750924557175),\n (3979, 0.023528750924557175),\n (3998, 0.023528750924557175),\n (3062, 0.02352806597262879),\n (4665, 0.02347121168493857),\n (3836, 0.02344472209317015),\n (4117, 0.0234045701625292),\n (2377, 0.023320734979997478),\n (1227, 0.023284131888018325),\n (791, 0.02325808241331919),\n (2920, 0.023252591658774052),\n (97, 0.02325054515231178),\n (147, 0.02325054515231178),\n (414, 0.02319135056230518),\n (581, 0.02319135056230518),\n ...]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "job_description = \"\"\"\n",
    "Системный администратор\n",
    "Знание и опыт работы с серверными операционными системами Windows Server и Linux.\n",
    "Умение настраивать сетевое оборудование.\n",
    "Опыт работы с системами виртуализации и облачными сервисами.\n",
    "Способность быстро решать проблемы с IT-инфраструктурой.\n",
    "Навыки работы с базами данных и системами резервного копирования.\n",
    "\"\"\"\n",
    "\n",
    "text_columns = ['Position', 'Specializations', 'Previous_Positions', 'Languages', 'Education', 'About_Me', 'Skills']\n",
    "\n",
    "resume_texts = df[text_columns].fillna('').agg(' '.join, axis=1).tolist()\n",
    "all_texts = [job_description] + resume_texts\n",
    "# all_texts = [preprocess_text(doc) for doc in all_texts]\n",
    "# all_texts= [\" \".join(doc) for doc in all_texts]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix_sysadmin_modified = tfidf_vectorizer.fit_transform(all_texts)\n",
    "\n",
    "cosine_similarities_tfidf = cosine_similarity(tfidf_matrix_sysadmin_modified[0:1],\n",
    "                                              tfidf_matrix_sysadmin_modified[1:]).flatten()\n",
    "\n",
    "resume_similarities_tfidf = dict(enumerate(cosine_similarities_tfidf))\n",
    "\n",
    "sorted_resume_similarities_tfidf = sorted(resume_similarities_tfidf.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "sorted_resume_similarities_tfidf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:58:41.871250700Z",
     "start_time": "2023-11-05T13:58:41.316467100Z"
    }
   },
   "id": "59b1a93df4375d84"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, 0.8560573714706771),\n (3747, 0.8560573714706771),\n (1020, 0.8367948828738325),\n (1350, 0.8367948828738325),\n (1470, 0.8367948828738325),\n (2276, 0.8000686934463266),\n (155, 0.7951884087151567),\n (4673, 0.7586763767030187),\n (1339, 0.7541173006654682),\n (69, 0.7499979758885262),\n (112, 0.7499979758885262),\n (1118, 0.7161947133635714),\n (1705, 0.7161947133635714),\n (1762, 0.7161947133635714),\n (3486, 0.714687832717636),\n (4625, 0.714687832717636),\n (2354, 0.6987013054523641),\n (4349, 0.6987013054523641),\n (4404, 0.6987013054523641),\n (1066, 0.6673887856943769),\n (2203, 0.6673887856943769),\n (2414, 0.6652230095381932),\n (3423, 0.6652230095381932),\n (2338, 0.6334822626768195),\n (4311, 0.6334822626768195),\n (4370, 0.596226100199497),\n (4427, 0.596226100199497),\n (810, 0.5906572539744138),\n (1185, 0.5700009018970741),\n (1613, 0.5700009018970741),\n (2035, 0.545588668830196),\n (3829, 0.5424102104856495),\n (971, 0.5422135036073212),\n (4651, 0.5399808000083213),\n (2118, 0.5119870216588283),\n (2190, 0.5119870216588283),\n (2294, 0.5107387618108016),\n (368, 0.4963669902543977),\n (4678, 0.4735302245647116),\n (1278, 0.47120121675534177),\n (1999, 0.47119662991107536),\n (59, 0.4681506537805675),\n (100, 0.4681506537805675),\n (490, 0.46534479551297886),\n (646, 0.46534479551297886),\n (386, 0.45211015818739253),\n (2978, 0.44318656924499267),\n (3952, 0.44318656924499267),\n (3517, 0.43784382887585127),\n (4751, 0.43381455987600914),\n (4743, 0.4321567896866135),\n (268, 0.43183549214869144),\n (303, 0.43183549214869144),\n (4718, 0.42570633471399627),\n (3527, 0.42491089772585605),\n (3775, 0.42376601879928477),\n (1299, 0.3980061284556836),\n (358, 0.37240955120724606),\n (2968, 0.35045286395188274),\n (3938, 0.35045286395188274),\n (4465, 0.34275362646813756),\n (4092, 0.34020899696686374),\n (3212, 0.3293706373107218),\n (4810, 0.321564077349425),\n (4763, 0.3188244828161906),\n (3968, 0.31500601720676574),\n (3987, 0.31500601720676574),\n (359, 0.31364781282395315),\n (382, 0.3000946655649185),\n (2513, 0.29067786929790884),\n (3349, 0.29067786929790884),\n (813, 0.2710891195049508),\n (657, 0.26921244232819824),\n (688, 0.26921244232819824),\n (4459, 0.26878160821972324),\n (3713, 0.25332956321316064),\n (4803, 0.24878424506161614),\n (2312, 0.24717706742957268),\n (3470, 0.2449243304599174),\n (4605, 0.2449243304599174),\n (4218, 0.24305293628439706),\n (983, 0.24256226081906962),\n (1349, 0.24256226081906962),\n (279, 0.24219714284000307),\n (318, 0.24219714284000307),\n (1844, 0.23132213901900026),\n (3847, 0.22933626125721787),\n (4502, 0.22248642584529799),\n (2103, 0.22032005057319085),\n (2173, 0.22032005057319085),\n (1245, 0.21643401117991887),\n (2435, 0.21564664447723642),\n (3236, 0.21564664447723642),\n (496, 0.21352608461701245),\n (651, 0.21352608461701245),\n (1865, 0.21130118559225536),\n (3458, 0.20998786174293826),\n (4586, 0.20998786174293826),\n (1813, 0.20892577653051206),\n (798, 0.19922382851153952),\n (3139, 0.19561967743383774),\n (4249, 0.19561967743383774),\n (4754, 0.19529439090774836),\n (1784, 0.19085884094429195),\n (51, 0.19027804356028488),\n (1433, 0.18962615062731858),\n (4760, 0.18811279751169116),\n (2075, 0.18725809384902806),\n (2144, 0.18725809384902806),\n (2732, 0.1845507259049944),\n (3545, 0.1845507259049944),\n (3592, 0.18200978514888988),\n (4094, 0.18157323705491202),\n (1089, 0.1793978511851215),\n (1663, 0.1793978511851215),\n (2348, 0.17751112026757163),\n (4339, 0.17751112026757163),\n (4398, 0.17751112026757163),\n (2992, 0.177384752487906),\n (3976, 0.177384752487906),\n (3991, 0.177384752487906),\n (2055, 0.17705615187144855),\n (3058, 0.17296696984240514),\n (2923, 0.172096207456177),\n (3294, 0.1708227551812808),\n (3299, 0.1708227551812808),\n (1227, 0.1701673344293514),\n (1236, 0.16830218743666914),\n (2783, 0.1676036079089422),\n (2867, 0.1659447020072928),\n (3743, 0.1658115270345163),\n (30, 0.15947371254307485),\n (61, 0.15860313861162015),\n (106, 0.15860313861162015),\n (1141, 0.1563478478578251),\n (1789, 0.1563478478578251),\n (241, 0.15398868556924966),\n (2714, 0.1456749923046582),\n (3528, 0.1456749923046582),\n (3489, 0.14567247692059845),\n (4626, 0.14567247692059845),\n (595, 0.145037083333388),\n (4130, 0.14314642944402375),\n (244, 0.1423294067675915),\n (4343, 0.14141043253231242),\n (4396, 0.14141043253231242),\n (70, 0.14107765056977487),\n (114, 0.14107765056977487),\n (3741, 0.13966695013264854),\n (3324, 0.13579749070418762),\n (527, 0.13578547633984867),\n (708, 0.13578547633984867),\n (792, 0.13547333632997619),\n (1169, 0.13185407871271804),\n (1597, 0.13185407871271804),\n (3440, 0.13005083267812684),\n (4563, 0.13005083267812684),\n (1325, 0.1292402274911447),\n (1114, 0.12732622218203835),\n (1694, 0.12732622218203835),\n (1743, 0.12732622218203835),\n (665, 0.12378116967989247),\n (690, 0.12378116967989247),\n (3222, 0.12357117584728317),\n (1973, 0.12290000482970649),\n (2612, 0.12290000482970649),\n (2691, 0.12290000482970649),\n (3946, 0.12042279685581435),\n (2974, 0.11800554745996308),\n (3949, 0.11800554745996308),\n (3346, 0.11770822540649953),\n (3443, 0.11335580569046318),\n (4569, 0.11335580569046318),\n (1241, 0.11246753737534848),\n (643, 0.11205322640704188),\n (3663, 0.10976038729310304),\n (3614, 0.10890874427191792),\n (841, 0.10764283614229925),\n (2760, 0.10730002710059297),\n (3578, 0.10730002710059297),\n (4482, 0.10624198280474023),\n (2375, 0.10430674577432178),\n (4564, 0.10417608581805074),\n (1073, 0.10376778296425673),\n (2210, 0.10376778296425673),\n (2389, 0.1033111370041113),\n (4457, 0.10313304466405554),\n (2953, 0.10238940872847813),\n (3916, 0.10238940872847813),\n (4438, 0.10232561814060828),\n (2291, 0.10211690644048411),\n (1463, 0.10181450573482542),\n (2307, 0.10153221456445412),\n (1329, 0.10125922058343144)]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выберем только ключевые поля для первичной фильтрации\n",
    "key_columns = ['Position', 'Specializations', 'Previous_Positions']\n",
    "key_texts = df[key_columns].fillna('').agg(' '.join, axis=1).tolist()\n",
    "all_texts = [preprocess_text(doc) for doc in all_texts]\n",
    "# all_texts= [\" \".join(doc) for doc in all_texts]\n",
    "# Векторизация ключевых полей\n",
    "tfidf_vectorizer_keys = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_matrix_keys = tfidf_vectorizer_keys.fit_transform(['системный администратор'] + key_texts)\n",
    "\n",
    "# Вычисление косинусного сходства для ключевых полей\n",
    "cosine_similarities_keys = cosine_similarity(tfidf_matrix_keys[0:1], tfidf_matrix_keys[1:]).flatten()\n",
    "\n",
    "# Отбор резюме с сходством выше порога (например, пороговое значение может быть установлено на уровне 0.1)\n",
    "threshold = 0.1\n",
    "preselected_resume_indices = [(index, similarity) for index, similarity in enumerate(cosine_similarities_keys) if similarity > threshold]\n",
    "sorted(preselected_resume_indices, key=lambda item: item[1], reverse=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T14:29:18.166824800Z",
     "start_time": "2023-11-05T14:29:17.862826200Z"
    }
   },
   "id": "3bb6bf87933a48b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Предполагается, что у вас есть предобученная модель FastText в файле 'fasttext.model.bin'\n",
    "# Загружаем предобученную модель FastText\n",
    "model = KeyedVectors.load_word2vec_format('cc.ru.300.bin', binary=True)\n",
    "\n",
    "# Функция для получения вектора документа путем усреднения векторов слов\n",
    "def document_vector(doc):\n",
    "    # Удаляем слова, которых нет в модели\n",
    "    words = [word for word in doc.split() if word in model.vocab]\n",
    "    if len(words) == 0:\n",
    "       return np.zeros(model.vector_size)\n",
    "    else:\n",
    "       # Усредняем векторы слов, чтобы получить вектор документа\n",
    "       return np.mean(model[words], axis=0)\n",
    "\n",
    "# Преобразуем описание вакансии и резюме в векторы\n",
    "job_vec = document_vector(job_description)\n",
    "resume_vecs = np.array([document_vector(doc) for doc in resume_texts])\n",
    "\n",
    "# Вычисляем косинусное сходство между вакансией и каждым резюме\n",
    "cosine_similarities = cosine_similarity([job_vec], resume_vecs).flatten()\n",
    "\n",
    "# Создаем словарь сопоставлений индекс резюме -> сходство\n",
    "resume_similarities = dict(enumerate(cosine_similarities))\n",
    "\n",
    "# Сортируем резюме по убыванию сходства\n",
    "sorted_resume_similarities = sorted(resume_similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "sorted_resume_similarities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5305b53794941772"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:43:53.741567500Z",
     "start_time": "2023-11-05T13:43:53.719539Z"
    }
   },
   "id": "75c3a935cc59b723"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  10%|█         | 1/10 [00:08<01:13,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  20%|██        | 2/10 [00:16<01:04,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  30%|███       | 3/10 [00:24<00:55,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  40%|████      | 4/10 [00:31<00:47,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  50%|█████     | 5/10 [00:39<00:39,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  60%|██████    | 6/10 [00:48<00:32,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  70%|███████   | 7/10 [00:55<00:23,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  80%|████████  | 8/10 [01:03<00:15,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  90%|█████████ | 9/10 [01:11<00:07,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs: 100%|██████████| 10/10 [01:18<00:00,  7.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_texts)]\n",
    "model = Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.00025, min_count=1, dm=0, seed=42)\n",
    "model.build_vocab(documents)\n",
    "\n",
    "for epoch in tqdm(range(10), desc=\"Training epochs\"):\n",
    "    print(f'Iteration {epoch}')\n",
    "    model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:45:13.107129300Z",
     "start_time": "2023-11-05T13:43:53.736538100Z"
    }
   },
   "id": "d567b1576f9a888c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "resume_texts_series = df[text_columns].fillna('').agg(' '.join, axis=1)\n",
    "resume_vecs_series = resume_texts_series.apply(lambda x: model.infer_vector(word_tokenize(x.lower()), epochs=20))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:45:19.522918200Z",
     "start_time": "2023-11-05T13:45:13.125128900Z"
    }
   },
   "id": "5d1b7877d7aa0949"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dforgeek\\AppData\\Local\\Temp\\ipykernel_5696\\4235598925.py:12: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  model.docvecs.most_similar([job_vector])\n"
     ]
    },
    {
     "data": {
      "text/plain": "[(3744, 0.5628329515457153),\n (2173, 0.5476540327072144),\n (3688, 0.5454566478729248),\n (3482, 0.5423063039779663),\n (3960, 0.5381333231925964),\n (700, 0.532794713973999),\n (2087, 0.531611442565918),\n (3276, 0.5315738320350647),\n (678, 0.5302934646606445),\n (2105, 0.5277499556541443)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "job_vector = model.infer_vector(word_tokenize(job_description.lower()), epochs=20)\n",
    "# resume_vecs = [model.infer_vector(word_tokenize(text.lower())) for text in resume_texts]\n",
    "\n",
    "#cosine_similarities_doc2vec = [cosine_similarity([job_vector], [resume_vec])[0][0] for resume_vec in resume_vecs]\n",
    "cosine_similarities_doc2vec = resume_vecs_series.apply(lambda x: cosine_similarity([job_vector], [x])[0][0])\n",
    "sorted_similarities = sorted(enumerate(cosine_similarities_doc2vec, 1), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Вывод результатов\n",
    "#print(sorted_similarities)\n",
    "\n",
    "\n",
    "model.docvecs.most_similar([job_vector]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:45:20.478794800Z",
     "start_time": "2023-11-05T13:45:19.526919800Z"
    }
   },
   "id": "7f434ea883188dff"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[(1117, 0.9829046726226807),\n (1505, 0.9808845520019531),\n (4771, 0.9804688096046448),\n (980, 0.9802522659301758),\n (48, 0.9794981479644775),\n (648, 0.9794855117797852),\n (3188, 0.9792640209197998),\n (3904, 0.9792094230651855),\n (35, 0.9791171550750732),\n (966, 0.9785678386688232),\n (2203, 0.9785106182098389),\n (2563, 0.9784299731254578),\n (3811, 0.9783999919891357),\n (1293, 0.978130578994751),\n (2105, 0.9781256914138794),\n (557, 0.9781102538108826),\n (119, 0.9780694246292114),\n (34, 0.9780210852622986),\n (1423, 0.9777166843414307),\n (1572, 0.9776190519332886),\n (434, 0.9776048064231873),\n (3128, 0.9775843620300293),\n (3442, 0.9774499535560608),\n (569, 0.9774157404899597),\n (1043, 0.9773767590522766),\n (848, 0.9773274660110474),\n (854, 0.9773245453834534),\n (2140, 0.9773141741752625),\n (1306, 0.9773106575012207),\n (2700, 0.9771040081977844),\n (1936, 0.9770858883857727),\n (1674, 0.9770616888999939),\n (4484, 0.9769256711006165),\n (1366, 0.9768769145011902),\n (3235, 0.9768640398979187),\n (3521, 0.9768174886703491),\n (3764, 0.976657509803772),\n (1730, 0.9765323996543884),\n (95, 0.9765126705169678),\n (1406, 0.9764755368232727),\n (775, 0.9764196276664734),\n (1920, 0.9763176441192627),\n (1993, 0.9763054847717285),\n (499, 0.9762757420539856),\n (1145, 0.9762359857559204),\n (4608, 0.9762260317802429),\n (4277, 0.9761804938316345),\n (404, 0.9760925769805908),\n (2426, 0.9760103821754456),\n (1894, 0.9759963750839233),\n (3117, 0.9758957028388977),\n (2859, 0.9758764505386353),\n (3219, 0.9756774306297302),\n (1136, 0.9756743311882019),\n (1802, 0.9756582379341125),\n (2776, 0.9756546020507812),\n (2431, 0.975651204586029),\n (736, 0.9756472706794739),\n (4065, 0.9756345152854919),\n (236, 0.9755585789680481),\n (659, 0.9754943251609802),\n (4469, 0.9754367470741272),\n (1054, 0.9754320383071899),\n (2409, 0.9753479361534119),\n (3708, 0.9753349423408508),\n (1913, 0.9753105640411377),\n (3071, 0.9752945303916931),\n (4183, 0.9752703309059143),\n (260, 0.9751907587051392),\n (2768, 0.9751151204109192),\n (2960, 0.9750136733055115),\n (4567, 0.9749975800514221),\n (740, 0.974948525428772),\n (1135, 0.9748836159706116),\n (4328, 0.9748664498329163),\n (1715, 0.9747787714004517),\n (748, 0.9747767448425293),\n (4102, 0.9745910167694092),\n (4364, 0.9745677709579468),\n (3896, 0.9745433926582336),\n (1747, 0.9745222330093384),\n (4774, 0.9745199084281921),\n (2791, 0.9745010137557983),\n (1275, 0.9744247794151306),\n (122, 0.9744022488594055),\n (4847, 0.9744018316268921),\n (637, 0.9743844866752625),\n (4100, 0.9743775129318237),\n (1349, 0.9742857813835144),\n (4257, 0.9742685556411743),\n (4147, 0.9742597341537476),\n (1386, 0.974239706993103),\n (3738, 0.9742066860198975),\n (2161, 0.9741992950439453),\n (1439, 0.9741818904876709),\n (2013, 0.9741021394729614),\n (1779, 0.9740962386131287),\n (3674, 0.9740834832191467),\n (860, 0.9740467071533203),\n (1057, 0.9740386009216309),\n (3, 0.9740116000175476),\n (4784, 0.9737844467163086),\n (4532, 0.9737839102745056),\n (4192, 0.9736695885658264),\n (2717, 0.9736410975456238),\n (3312, 0.9736168384552002),\n (2795, 0.9735656380653381),\n (3490, 0.9734786748886108),\n (4735, 0.9734699130058289),\n (3587, 0.9734642505645752),\n (985, 0.9734342694282532),\n (2210, 0.9734112620353699),\n (2270, 0.9733436703681946),\n (3187, 0.9731931686401367),\n (1409, 0.9731644988059998),\n (2516, 0.972991943359375),\n (3518, 0.9729725122451782),\n (1880, 0.9729580879211426),\n (1666, 0.9729474186897278),\n (2615, 0.9729217290878296),\n (1994, 0.9728984236717224),\n (3625, 0.9728514552116394),\n (1799, 0.97282475233078),\n (325, 0.9728186130523682),\n (912, 0.9728053212165833),\n (2316, 0.9727906584739685),\n (247, 0.9727360606193542),\n (3753, 0.9727022647857666),\n (2535, 0.9726863503456116),\n (2927, 0.9726691842079163),\n (3376, 0.9726390838623047),\n (4829, 0.9726333618164062),\n (142, 0.9726014137268066),\n (3759, 0.9725936055183411),\n (2029, 0.9725832939147949),\n (2582, 0.9725736379623413),\n (4092, 0.972470760345459),\n (2489, 0.9724594950675964),\n (3316, 0.9724406003952026),\n (4655, 0.9724345207214355),\n (866, 0.9724093079566956),\n (1007, 0.9723873734474182),\n (4229, 0.972366452217102),\n (1685, 0.9723648428916931),\n (1002, 0.9723602533340454),\n (4627, 0.9723269939422607),\n (4262, 0.9722424745559692),\n (1549, 0.9722262620925903),\n (1086, 0.9722214937210083),\n (2767, 0.9722213745117188),\n (1415, 0.9722186326980591),\n (720, 0.9721925258636475),\n (1969, 0.9721769094467163),\n (1540, 0.9721400141716003),\n (2723, 0.9721248745918274),\n (1863, 0.972114622592926),\n (3500, 0.9721136689186096),\n (2019, 0.9721047282218933),\n (3539, 0.9720802307128906),\n (2572, 0.9720458388328552),\n (4308, 0.971978485584259),\n (2343, 0.9719098210334778),\n (4282, 0.9718149900436401),\n (1148, 0.9718082547187805),\n (2712, 0.9717898964881897),\n (4386, 0.9717716574668884),\n (24, 0.9717530012130737),\n (2586, 0.9717119932174683),\n (1918, 0.971701443195343),\n (3386, 0.9716320037841797),\n (2504, 0.9716281890869141),\n (4703, 0.9715794920921326),\n (26, 0.9715601205825806),\n (68, 0.9715364575386047),\n (759, 0.9715166687965393),\n (1517, 0.9714736342430115),\n (2887, 0.9714153409004211),\n (1257, 0.9713972806930542),\n (2088, 0.9713387489318848),\n (77, 0.97133469581604),\n (651, 0.9712888598442078),\n (2152, 0.9712271690368652),\n (708, 0.9711816310882568),\n (199, 0.9711179733276367),\n (442, 0.9711049795150757),\n (4216, 0.9710732698440552),\n (2243, 0.9710660576820374),\n (843, 0.971054196357727),\n (618, 0.9710438251495361),\n (2888, 0.9710211157798767),\n (3406, 0.9710210561752319),\n (1402, 0.9709657430648804),\n (2033, 0.9709210991859436),\n (914, 0.9708742499351501),\n (603, 0.9708650708198547),\n (4799, 0.9708524346351624),\n (208, 0.9708431363105774),\n (3583, 0.9708371758460999),\n (1985, 0.9708370566368103),\n (2746, 0.9708205461502075),\n (650, 0.9708147644996643),\n (3835, 0.9708011746406555),\n (4038, 0.9707825779914856),\n (4632, 0.97074294090271),\n (3541, 0.9707200527191162),\n (3558, 0.9707191586494446),\n (550, 0.9706289172172546),\n (1091, 0.9706109762191772),\n (3148, 0.9705806374549866),\n (1330, 0.9705367088317871),\n (2816, 0.9705188870429993),\n (1586, 0.9704957604408264),\n (3767, 0.9704815745353699),\n (1162, 0.9704393148422241),\n (4292, 0.9704389572143555),\n (1208, 0.9704388976097107),\n (1254, 0.9704122543334961),\n (184, 0.9704000949859619),\n (536, 0.970342218875885),\n (394, 0.9703349471092224),\n (631, 0.9703111052513123),\n (3201, 0.9703004360198975),\n (308, 0.9702905416488647),\n (4209, 0.9702892303466797),\n (1163, 0.9702528119087219),\n (1754, 0.9702498316764832),\n (3785, 0.9702441096305847),\n (2583, 0.9701675772666931),\n (2664, 0.9701420664787292),\n (3607, 0.97011798620224),\n (1105, 0.9701076149940491),\n (2856, 0.97010338306427),\n (1637, 0.9700415730476379),\n (3839, 0.9700415134429932),\n (3434, 0.9700334072113037),\n (4832, 0.9700026512145996),\n (1485, 0.9699940085411072),\n (1566, 0.9699825644493103),\n (3489, 0.9699776768684387),\n (821, 0.969961941242218),\n (3621, 0.9699603915214539),\n (3958, 0.9699577689170837),\n (3599, 0.9699512124061584),\n (3306, 0.9699302911758423),\n (55, 0.9699296355247498),\n (3854, 0.9699268937110901),\n (4128, 0.9699251651763916),\n (3200, 0.9698716402053833),\n (1484, 0.9698675870895386),\n (1083, 0.9698548316955566),\n (2173, 0.9698021411895752),\n (4378, 0.9697789549827576),\n (1855, 0.9696587920188904),\n (1824, 0.9696317315101624),\n (1458, 0.9696282744407654),\n (635, 0.9696138501167297),\n (1914, 0.9696084856987),\n (4664, 0.9695998430252075),\n (338, 0.9695943593978882),\n (2878, 0.9695476293563843),\n (1461, 0.9695410132408142),\n (2179, 0.9695329070091248),\n (792, 0.9695020914077759),\n (2070, 0.9694831967353821),\n (1155, 0.9694624543190002),\n (3743, 0.9694379568099976),\n (2137, 0.9694324731826782),\n (987, 0.9694302082061768),\n (1826, 0.9694239497184753),\n (4330, 0.9694129824638367),\n (2023, 0.9693798422813416),\n (3469, 0.969350278377533),\n (1420, 0.9693500995635986),\n (4322, 0.9693158864974976),\n (785, 0.9693052172660828),\n (2732, 0.9692751169204712),\n (2404, 0.9692490696907043),\n (896, 0.9692447781562805),\n (1331, 0.9692202806472778),\n (3585, 0.9692152738571167),\n (3525, 0.9692139029502869),\n (2093, 0.9691785573959351),\n (3861, 0.9691426753997803),\n (1628, 0.9691218733787537),\n (1973, 0.9691136479377747),\n (614, 0.9691018462181091),\n (419, 0.9690657258033752),\n (1099, 0.9690478444099426),\n (4243, 0.9690252542495728),\n (1308, 0.9690167903900146),\n (3683, 0.9690154790878296),\n (367, 0.9689983129501343),\n (691, 0.9689912796020508),\n (2893, 0.9689406156539917),\n (1055, 0.9689152240753174),\n (212, 0.9688800573348999),\n (2240, 0.9688684344291687),\n (2132, 0.9688372611999512),\n (1404, 0.9688253998756409),\n (2064, 0.9688058495521545),\n (4290, 0.9687945246696472),\n (4113, 0.9687394499778748),\n (3056, 0.968730628490448),\n (2507, 0.9687095880508423),\n (3190, 0.9686925411224365),\n (745, 0.9686170220375061),\n (2072, 0.9685947895050049),\n (2790, 0.9685783386230469),\n (3016, 0.9685609340667725),\n (1425, 0.9685442447662354),\n (2637, 0.968543291091919),\n (4739, 0.9685235619544983),\n (274, 0.9684970378875732),\n (2124, 0.9684645533561707),\n (1991, 0.968430757522583),\n (4032, 0.9684246182441711),\n (3412, 0.9684179425239563),\n (351, 0.9684163928031921),\n (3314, 0.9684089422225952),\n (479, 0.9684044718742371),\n (1102, 0.9683945178985596),\n (1216, 0.9683784246444702),\n (689, 0.9683722853660583),\n (4823, 0.9683544039726257),\n (4028, 0.9683409333229065),\n (2380, 0.9683313965797424),\n (2934, 0.9683181047439575),\n (4358, 0.9682884216308594),\n (993, 0.9682400226593018),\n (3995, 0.9682387113571167),\n (1539, 0.9682362079620361),\n (4441, 0.9681499600410461),\n (3883, 0.9681240916252136),\n (492, 0.9681088924407959),\n (3779, 0.968100905418396),\n (4586, 0.9680579304695129),\n (2774, 0.9680364727973938),\n (4326, 0.9680178165435791),\n (2395, 0.9680124521255493),\n (3182, 0.9680103659629822),\n (1519, 0.9680019021034241),\n (4063, 0.9679937362670898),\n (3400, 0.9679890275001526),\n (2191, 0.9679753184318542),\n (2548, 0.9679662585258484),\n (1291, 0.9679343700408936),\n (2411, 0.9679326415061951),\n (3979, 0.9679173827171326),\n (3464, 0.9678356051445007),\n (633, 0.9678001403808594),\n (2119, 0.9677693843841553),\n (2531, 0.9677403569221497),\n (3419, 0.9677403569221497),\n (4828, 0.9677236080169678),\n (2493, 0.9676483869552612),\n (3324, 0.9676209688186646),\n (46, 0.9676175117492676),\n (2163, 0.9676131010055542),\n (2523, 0.9676080346107483),\n (2690, 0.9675772190093994),\n (361, 0.9675689935684204),\n (1175, 0.9675270318984985),\n (3399, 0.9675183892250061),\n (1364, 0.9674963355064392),\n (601, 0.967484176158905),\n (3317, 0.9674782156944275),\n (3472, 0.9674485921859741),\n (2646, 0.9674344062805176),\n (1885, 0.9674239754676819),\n (3952, 0.9674035906791687),\n (2744, 0.9673993587493896),\n (1605, 0.9673671722412109),\n (4536, 0.9673636555671692),\n (2638, 0.9673540592193604),\n (3026, 0.9673435688018799),\n (2390, 0.9673115015029907),\n (330, 0.9672495722770691),\n (152, 0.9672262668609619),\n (4708, 0.9671902060508728),\n (4224, 0.9671733975410461),\n (2391, 0.9671725034713745),\n (3685, 0.9671723246574402),\n (823, 0.9671677350997925),\n (1675, 0.9670649766921997),\n (4833, 0.9670465588569641),\n (2212, 0.9670147895812988),\n (41, 0.9669702649116516),\n (3356, 0.966946005821228),\n (248, 0.9669108390808105),\n (4220, 0.9668375253677368),\n (3878, 0.9668353796005249),\n (3869, 0.9668321013450623),\n (2177, 0.9668288826942444),\n (4413, 0.9668092727661133),\n (4445, 0.9668018817901611),\n (2021, 0.9667736887931824),\n (2400, 0.9667305946350098),\n (3538, 0.966711699962616),\n (2461, 0.9666361212730408),\n (495, 0.9666313529014587),\n (4458, 0.9666191339492798),\n (4139, 0.966588020324707),\n (3760, 0.9665621519088745),\n (1793, 0.9665619730949402),\n (991, 0.9665218591690063),\n (3602, 0.9665157794952393),\n (1938, 0.9664930105209351),\n (1011, 0.9664429426193237),\n (1981, 0.9664165377616882),\n (1643, 0.9663757681846619),\n (1764, 0.9663125276565552),\n (4079, 0.9663053750991821),\n (3220, 0.9663038849830627),\n (2824, 0.9662998914718628),\n (423, 0.9662997126579285),\n (3931, 0.9662593603134155),\n (2454, 0.9662355184555054),\n (1079, 0.9662163257598877),\n (582, 0.9662056565284729),\n (2359, 0.966198742389679),\n (3775, 0.9661777019500732),\n (2708, 0.9661478996276855),\n (527, 0.9661433696746826),\n (2410, 0.9661102294921875),\n (3690, 0.9660570025444031),\n (3224, 0.9660463333129883),\n (1217, 0.9660435318946838),\n (3797, 0.9660273194313049),\n (1806, 0.9660195112228394),\n (4812, 0.9660102725028992),\n (4631, 0.9659872055053711),\n (1905, 0.9659653902053833),\n (2571, 0.9659476280212402),\n (716, 0.9659361243247986),\n (4768, 0.9659346342086792),\n (2942, 0.9658774733543396),\n (2642, 0.9658662676811218),\n (1750, 0.9658210277557373),\n (383, 0.9657753109931946),\n (2366, 0.9657625555992126),\n (315, 0.965762197971344),\n (473, 0.9657396674156189),\n (1076, 0.9657291769981384),\n (3481, 0.9657273888587952),\n (4788, 0.9656974673271179),\n (538, 0.9656944274902344),\n (2034, 0.965690553188324),\n (850, 0.9656826853752136),\n (1601, 0.9656756520271301),\n (2832, 0.9656727910041809),\n (78, 0.965663731098175),\n (1170, 0.9656564593315125),\n (3227, 0.9656526446342468),\n (1699, 0.9656323790550232),\n (4604, 0.9656158089637756),\n (1363, 0.9655800461769104),\n (2158, 0.9655783176422119),\n (3264, 0.9655709862709045),\n (3829, 0.9655599594116211),\n (3100, 0.9655285477638245),\n (4512, 0.96550452709198),\n (86, 0.965473473072052),\n (2095, 0.965473473072052),\n (36, 0.9654717445373535),\n (737, 0.9654581546783447),\n (2215, 0.9654581546783447),\n (1010, 0.9654459357261658),\n (2492, 0.9654229283332825),\n (1030, 0.9654202461242676),\n (511, 0.965410053730011),\n (4806, 0.9653933644294739),\n (2780, 0.9653835892677307),\n (2836, 0.9653741121292114),\n (2099, 0.9653712511062622),\n (2363, 0.9653515815734863),\n (1249, 0.9653432965278625),\n (4122, 0.9653383493423462),\n (3444, 0.9653246402740479),\n (1948, 0.965300440788269),\n (2687, 0.9652583003044128),\n (712, 0.9652459025382996),\n (1658, 0.965242862701416),\n (4783, 0.965237021446228),\n (863, 0.9652316570281982),\n (2090, 0.9652222990989685),\n (45, 0.9652199149131775),\n (849, 0.9651475548744202),\n (161, 0.9651056528091431),\n (2017, 0.9650998711585999),\n (333, 0.9650990962982178),\n (3964, 0.9650937914848328),\n (101, 0.9650758504867554),\n (3410, 0.9650581479072571),\n (3563, 0.9650560617446899),\n (162, 0.9650457501411438),\n (2046, 0.9649991393089294),\n (2754, 0.9649881720542908),\n (2703, 0.964988112449646),\n (1150, 0.964979350566864),\n (4409, 0.9649108052253723),\n (1536, 0.9649064540863037),\n (4379, 0.9648942947387695),\n (2245, 0.9648892283439636),\n (3695, 0.9648757576942444),\n (3170, 0.9648743867874146),\n (933, 0.9648741483688354),\n (4076, 0.9648380279541016),\n (762, 0.9648350477218628),\n (3508, 0.9647958874702454),\n (4714, 0.964786946773529),\n (4214, 0.9647670984268188),\n (4238, 0.9646461606025696),\n (2813, 0.9646337628364563),\n (2984, 0.9646247029304504),\n (1659, 0.9646134972572327),\n (2485, 0.9646109938621521),\n (705, 0.964601457118988),\n (2633, 0.9645954966545105),\n (2990, 0.9645814895629883),\n (2268, 0.9645612239837646),\n (1728, 0.9645446538925171),\n (3897, 0.9645349383354187),\n (4387, 0.9645190238952637),\n (13, 0.9644701480865479),\n (4455, 0.9644630551338196),\n (946, 0.9644538760185242),\n (931, 0.9644527435302734),\n (4499, 0.964424192905426),\n (3323, 0.9644216895103455),\n (4705, 0.9643962383270264),\n (2389, 0.9643874168395996),\n (2904, 0.9643702507019043),\n (3498, 0.9643023014068604),\n (2593, 0.9643020629882812),\n (3079, 0.9642841219902039),\n (3176, 0.9642539620399475),\n (4817, 0.9642421007156372),\n (865, 0.9642117619514465),\n (828, 0.9642040729522705),\n (1999, 0.9641767144203186),\n (4590, 0.9641618132591248),\n (92, 0.9641445279121399),\n (3729, 0.9641430377960205),\n (588, 0.9641385078430176),\n (1070, 0.9641333818435669),\n (281, 0.9641145467758179),\n (3023, 0.9640881419181824),\n (4373, 0.9640874862670898),\n (1309, 0.9640775918960571),\n (3217, 0.9640746116638184),\n (3496, 0.9640660285949707),\n (4195, 0.9640647768974304),\n (2835, 0.9640436768531799),\n (807, 0.9640378355979919),\n (1522, 0.9640355706214905),\n (136, 0.9640042781829834),\n (1792, 0.9639937281608582),\n (1531, 0.9639776945114136),\n (3654, 0.963972270488739),\n (2433, 0.9639536738395691),\n (3042, 0.9639432430267334),\n (2648, 0.9639386534690857),\n (1194, 0.9639312028884888),\n (2546, 0.9639228582382202),\n (4256, 0.9639213681221008),\n (521, 0.963917076587677),\n (686, 0.9639104008674622),\n (1, 0.9639078378677368),\n (3624, 0.9638744592666626),\n (4438, 0.9638288021087646),\n (507, 0.9638069868087769),\n (3337, 0.963803231716156),\n (3505, 0.9638018608093262),\n (690, 0.9637957215309143),\n (246, 0.9637793898582458),\n (3513, 0.9637383222579956),\n (1305, 0.9637230634689331),\n (880, 0.9636854529380798),\n (3694, 0.9636783003807068),\n (810, 0.9636757373809814),\n (2863, 0.9636586904525757),\n (3335, 0.9636532068252563),\n (3772, 0.9636172652244568),\n (3218, 0.9636150598526001),\n (3222, 0.9635955095291138),\n (1087, 0.9635884761810303),\n (3842, 0.9635660648345947),\n (3493, 0.9635584950447083),\n (2543, 0.9635412096977234),\n (2202, 0.9634557366371155),\n (3184, 0.9634433388710022),\n (3333, 0.9634349942207336),\n (2650, 0.9634310007095337),\n (1559, 0.9634161591529846),\n (2284, 0.9634103178977966),\n (50, 0.9634081125259399),\n (3083, 0.9634063839912415),\n (491, 0.9633932113647461),\n (3075, 0.9633763432502747),\n (3763, 0.9633566737174988),\n (2350, 0.9633179306983948),\n (4546, 0.9633049368858337),\n (2397, 0.9632440805435181),\n (1459, 0.9632419347763062),\n (519, 0.9632307887077332),\n (579, 0.9632151126861572),\n (4591, 0.9632110595703125),\n (2112, 0.9631962180137634),\n (3766, 0.9631869792938232),\n (3072, 0.9631837606430054),\n (2487, 0.9631702899932861),\n (2679, 0.9631553888320923),\n (629, 0.9631538391113281),\n (3273, 0.9630882740020752),\n (2165, 0.9630551338195801),\n (4420, 0.9630531072616577),\n (660, 0.9630191922187805),\n (4234, 0.9629948735237122),\n (1026, 0.9629548192024231),\n (4203, 0.9629536271095276),\n (2295, 0.9629309177398682),\n (2108, 0.9629169702529907),\n (44, 0.9629131555557251),\n (294, 0.9629061222076416),\n (2559, 0.9629058837890625),\n (2038, 0.9628971815109253),\n (437, 0.9628925919532776),\n (3122, 0.9628850221633911),\n (3081, 0.9628844857215881),\n (1066, 0.9628841876983643),\n (3115, 0.9628517627716064),\n (4278, 0.9628502726554871),\n (2876, 0.9628469944000244),\n (4509, 0.9628308415412903),\n (2775, 0.9627664089202881),\n (4449, 0.9627297520637512),\n (4666, 0.9627217650413513),\n (2096, 0.9627030491828918),\n (1222, 0.9626871943473816),\n (116, 0.9626861810684204),\n (1942, 0.9626721143722534),\n (2762, 0.9626489281654358),\n (3309, 0.9626115560531616),\n (2755, 0.9626108407974243),\n (138, 0.9626062512397766),\n (555, 0.9625715613365173),\n (1477, 0.9625642895698547),\n (2037, 0.9625582098960876),\n (4794, 0.9625481367111206),\n (4355, 0.9625366926193237),\n (1101, 0.9625351428985596),\n (1916, 0.9625303149223328),\n (2925, 0.9625154733657837),\n (87, 0.9625146389007568),\n (3950, 0.9624817967414856),\n (953, 0.9624636173248291),\n (4221, 0.9624564051628113),\n (1518, 0.9624544978141785),\n (767, 0.9624496102333069),\n (738, 0.9624474048614502),\n (4227, 0.9624281525611877),\n (3520, 0.9624091386795044),\n (3837, 0.9624063968658447),\n (2562, 0.9624026417732239),\n (1301, 0.9623661041259766),\n (1597, 0.9623556733131409),\n (3384, 0.9623436331748962),\n (2321, 0.9623429775238037),\n (4766, 0.9623157978057861),\n (213, 0.9622614979743958),\n (2299, 0.9622443914413452),\n (1035, 0.9622165560722351),\n (901, 0.9621893167495728),\n (576, 0.962181568145752),\n (2052, 0.962178647518158),\n (2336, 0.9621729254722595),\n (475, 0.9621687531471252),\n (2660, 0.96209716796875),\n (819, 0.9620958566665649),\n (3461, 0.9620856642723083),\n (228, 0.9620704650878906),\n (663, 0.9620609879493713),\n (3800, 0.9620416760444641),\n (4801, 0.962038516998291),\n (3809, 0.9620085954666138),\n (4641, 0.9619877934455872),\n (2764, 0.96195387840271),\n (2862, 0.9618510603904724),\n (3062, 0.9618355631828308),\n (395, 0.9618256092071533),\n (60, 0.9618086218833923),\n (1430, 0.9618005156517029),\n (2503, 0.9618003964424133),\n (4218, 0.9617832899093628),\n (4557, 0.9617491960525513),\n (171, 0.9617406129837036),\n (2406, 0.9616856575012207),\n (4017, 0.9616784453392029),\n (4353, 0.9616584181785583),\n (1660, 0.9616471529006958),\n (3830, 0.9616267085075378),\n (4320, 0.9616168737411499),\n (3135, 0.9616124033927917),\n (1199, 0.9616073369979858),\n (1575, 0.96160489320755),\n (4250, 0.961597204208374),\n (3730, 0.9615718126296997),\n (1957, 0.9615224599838257),\n (548, 0.9614997506141663),\n (3833, 0.9614697694778442),\n (1933, 0.9614694714546204),\n (2434, 0.9614647030830383),\n (415, 0.9614466428756714),\n (1835, 0.9613910913467407),\n (1126, 0.961358904838562),\n (2074, 0.9613332748413086),\n (3598, 0.9613243341445923),\n (3236, 0.9612818360328674),\n (3254, 0.9612792134284973),\n (3803, 0.9612380266189575),\n (3339, 0.9612190127372742),\n (4719, 0.9612155556678772),\n (3642, 0.9612087607383728),\n (1246, 0.9611849784851074),\n (459, 0.9611489176750183),\n (4376, 0.9611400961875916),\n (532, 0.9611367583274841),\n (3281, 0.961111307144165),\n (406, 0.9611037373542786),\n (4254, 0.9611018896102905),\n (928, 0.9611011743545532),\n (1919, 0.9610887765884399),\n (143, 0.9610570073127747),\n (2606, 0.9610008597373962),\n (2274, 0.9609732627868652),\n (431, 0.9609687328338623),\n (3629, 0.9609673619270325),\n (568, 0.9609405994415283),\n (2447, 0.9609385132789612),\n (243, 0.9608951807022095),\n (3140, 0.9608641862869263),\n (1056, 0.960857093334198),\n (3534, 0.9608559012413025),\n (2167, 0.960847020149231),\n (1283, 0.9608434438705444),\n (1464, 0.9608368873596191),\n (2472, 0.9608029127120972),\n (4061, 0.9607884287834167),\n (2923, 0.9607734680175781),\n (3168, 0.9607560634613037),\n (2148, 0.9607427716255188),\n (1944, 0.9607263207435608),\n (1707, 0.9606986045837402),\n (2219, 0.9606942534446716),\n (4704, 0.9606893062591553),\n (4844, 0.9606834053993225),\n (2686, 0.9606272578239441),\n (3543, 0.9606214165687561),\n (18, 0.9606081247329712),\n (2044, 0.9606066942214966),\n (4301, 0.9605970978736877),\n (2758, 0.9605900049209595),\n (4395, 0.9605687260627747),\n (1523, 0.9605647325515747),\n (3261, 0.9605487585067749),\n (2497, 0.9605469107627869),\n (645, 0.9605452418327332),\n (3659, 0.9605291485786438),\n (847, 0.960529088973999),\n (1676, 0.9605205059051514),\n (1888, 0.9604968428611755),\n (715, 0.9604911208152771),\n (4030, 0.9604809880256653),\n (2977, 0.960476815700531),\n (2385, 0.9604114890098572),\n (1949, 0.9604090452194214),\n (2574, 0.960331916809082),\n (1983, 0.9603299498558044),\n (4658, 0.9603255391120911),\n (3647, 0.960317075252533),\n (833, 0.9602980017662048),\n (4342, 0.9602863788604736),\n (235, 0.960286021232605),\n (4490, 0.9602859616279602),\n (4366, 0.9602721929550171),\n (2180, 0.9602248072624207),\n (19, 0.9602131247520447),\n (3112, 0.960197925567627),\n (2289, 0.960168182849884),\n (3491, 0.9601569771766663),\n (3719, 0.9601544141769409),\n (1166, 0.9601404070854187),\n (4323, 0.9601375460624695),\n (4808, 0.9601338505744934),\n (2101, 0.9600814580917358),\n (32, 0.9600375890731812),\n (4672, 0.9600300788879395),\n (1717, 0.9600163102149963),\n (4035, 0.9599929451942444),\n (1551, 0.959945023059845),\n (3651, 0.959908127784729),\n (1261, 0.9598944783210754),\n (3641, 0.9598559737205505),\n (1332, 0.9598501324653625),\n (3742, 0.9598037600517273),\n (2331, 0.9597994685173035),\n (956, 0.9597675204277039),\n (3375, 0.9597477912902832),\n (613, 0.9597246646881104),\n (1399, 0.9597166180610657),\n (2946, 0.9596496224403381),\n (3377, 0.9596430063247681),\n (4839, 0.9596362709999084),\n (2565, 0.9596301913261414),\n (2242, 0.9596232175827026),\n (126, 0.9596033096313477),\n (2429, 0.9595767855644226),\n (4709, 0.959575355052948),\n (756, 0.959571361541748),\n (2232, 0.9595323204994202),\n (53, 0.9595076441764832),\n (3177, 0.9595063924789429),\n (4552, 0.959492027759552),\n (1929, 0.9594516754150391),\n (1015, 0.9594423174858093),\n (1347, 0.959432065486908),\n (392, 0.9594115614891052),\n (997, 0.9594111442565918),\n (4791, 0.959378719329834),\n (3298, 0.9593743085861206),\n (2499, 0.9593451023101807),\n (4244, 0.9593328833580017),\n (4616, 0.9593312740325928),\n (2624, 0.9593148231506348),\n (3308, 0.9593043327331543),\n (4488, 0.9593012928962708),\n (4811, 0.9592605829238892),\n (3194, 0.9592496156692505),\n (2538, 0.959206759929657),\n (4003, 0.9592026472091675),\n (4617, 0.9591979384422302),\n (3473, 0.959182620048523),\n (4004, 0.9591503739356995),\n (4797, 0.9591385722160339),\n (1302, 0.9591256380081177),\n (2896, 0.9591068029403687),\n (1374, 0.9590883255004883),\n (1025, 0.9590628147125244),\n (1941, 0.9590520262718201),\n (3684, 0.9590514302253723),\n (3744, 0.9590426683425903),\n (2823, 0.9590423107147217),\n (2752, 0.9590350985527039),\n (1705, 0.959021806716919),\n (372, 0.9589894413948059),\n (2111, 0.9589704275131226),\n (1391, 0.9589638113975525),\n (4194, 0.9589575529098511),\n (3212, 0.9589187502861023),\n (1891, 0.958894670009613),\n (4786, 0.9588809013366699),\n (2125, 0.9588802456855774),\n (3107, 0.9588656425476074),\n (103, 0.9588526487350464),\n (1923, 0.9588302373886108),\n (2665, 0.9588270783424377),\n (3120, 0.9588173031806946),\n (1432, 0.9587785005569458),\n (3479, 0.9587610960006714),\n (2061, 0.9587594270706177),\n (628, 0.9587557315826416),\n (4407, 0.9587329030036926),\n (887, 0.95870441198349),\n (3556, 0.9586998820304871),\n (3005, 0.9586930274963379),\n (3118, 0.9586867094039917),\n (3713, 0.9586867094039917),\n (3927, 0.9586798548698425),\n (770, 0.9586667418479919),\n (3210, 0.9586405158042908),\n (2804, 0.9586362242698669),\n (1211, 0.9585747122764587),\n (870, 0.9585634469985962),\n (1594, 0.9585443139076233),\n (3712, 0.9585422873497009),\n (2867, 0.9585157036781311),\n (2873, 0.9585082530975342),\n (216, 0.9584951400756836),\n (1120, 0.9584943056106567),\n (3106, 0.9584656357765198),\n (1262, 0.9584314227104187),\n (3921, 0.9584105610847473),\n (3783, 0.9584041833877563),\n (4645, 0.9584029316902161),\n (2354, 0.9583649039268494),\n (2661, 0.9583407044410706),\n (3132, 0.9583306908607483),\n (2227, 0.9582831859588623),\n (3596, 0.9582769870758057),\n (4724, 0.9582669138908386),\n (3398, 0.9582498073577881),\n (3851, 0.9582440257072449),\n (1502, 0.9582098722457886),\n (1252, 0.9582052230834961),\n (3318, 0.958175778388977),\n (4215, 0.9581729769706726),\n (2178, 0.9581591486930847),\n (683, 0.958102822303772),\n (3475, 0.9580841064453125),\n (483, 0.9580831527709961),\n (2475, 0.9580519199371338),\n (2376, 0.9580345153808594),\n (2567, 0.9579881429672241),\n (1480, 0.9579794406890869),\n (1701, 0.9579300284385681),\n (215, 0.9579257369041443),\n (2647, 0.9578564167022705),\n (684, 0.9578526020050049),\n (1910, 0.9578487277030945),\n (4605, 0.9578066468238831),\n (224, 0.9578016400337219),\n (4752, 0.9577926993370056),\n (4588, 0.9577881097793579),\n (1598, 0.9577592015266418),\n (4169, 0.9577531814575195),\n (4560, 0.9577482342720032),\n (704, 0.9577478170394897),\n (391, 0.9577277302742004),\n (787, 0.9577210545539856),\n (4367, 0.9577047228813171),\n (4253, 0.957702100276947),\n (904, 0.9576998353004456),\n (2920, 0.9576812982559204),\n (477, 0.9576692581176758),\n (1442, 0.9576663374900818),\n (4419, 0.9576501846313477),\n (1031, 0.957642674446106),\n (2, 0.957632303237915),\n (2168, 0.9576321244239807),\n (4000, 0.9576314687728882),\n (4439, 0.9576007127761841),\n (3726, 0.9575640559196472),\n (3391, 0.9575636982917786),\n (4172, 0.9575608372688293),\n (4268, 0.9575530290603638),\n (3455, 0.9575511813163757),\n (3089, 0.9575455188751221),\n (238, 0.957533061504364),\n (795, 0.9574936032295227),\n (4105, 0.9574829339981079),\n (3589, 0.9574611783027649),\n (3341, 0.9574103951454163),\n (111, 0.9573806524276733),\n (2557, 0.9573726058006287),\n (4759, 0.9573400616645813),\n (3864, 0.9573327898979187),\n (874, 0.957308292388916),\n (2318, 0.9572848677635193),\n (3822, 0.9572780728340149),\n (3000, 0.957233726978302),\n (3139, 0.9572250247001648),\n (4086, 0.9572209119796753),\n (2982, 0.9572001695632935),\n (3084, 0.9571908116340637),\n (4236, 0.9571589827537537),\n (1052, 0.9571517109870911),\n (3401, 0.9571456909179688),\n (4838, 0.9570853114128113),\n (3446, 0.9570780992507935),\n (1203, 0.9570751786231995),\n (3994, 0.9570728540420532),\n (2936, 0.9570707678794861),\n (4814, 0.9570323824882507),\n (972, 0.9569877982139587),\n (3453, 0.9569804072380066),\n (4804, 0.9569408297538757),\n (2080, 0.9569270610809326),\n (4556, 0.9569020867347717),\n (11, 0.9568918347358704),\n (4377, 0.9568878412246704),\n (2473, 0.9568812251091003),\n (4158, 0.9568796157836914),\n (2676, 0.9568256139755249),\n (3646, 0.9568064212799072),\n (4463, 0.9567646384239197),\n (1255, 0.95676189661026),\n (453, 0.9567436575889587),\n (647, 0.9567089080810547),\n (2114, 0.9567086100578308),\n (2190, 0.9566789269447327),\n (4082, 0.9566506147384644),\n (2110, 0.9566465616226196),\n (2999, 0.9566330909729004),\n (2402, 0.9566265344619751),\n (1819, 0.9566249847412109),\n (1214, 0.9566203951835632),\n (2214, 0.9566033482551575),\n (329, 0.956591010093689),\n (3362, 0.9565855860710144),\n (1796, 0.956563413143158),\n ...]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_similarities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:45:20.551152400Z",
     "start_time": "2023-11-05T13:45:20.480800Z"
    }
   },
   "id": "2581250df58b8b42"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0.89810723"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = model.infer_vector(word_tokenize(resume_texts_series.loc[802].lower()), epochs=20)\n",
    "vec2 = model.infer_vector(word_tokenize(job_description.lower()), epochs=20)\n",
    "cosine_similarity([vec1], [vec2])[0][0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T13:45:20.635731300Z",
     "start_time": "2023-11-05T13:45:20.561154500Z"
    }
   },
   "id": "12d78d8012219da5"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[81], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m#Load AutoModel from huggingface model repository\u001B[39;00m\n\u001B[0;32m     14\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mai-forever/sbert_large_mt_nlu_ru\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 15\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mai-forever/sbert_large_mt_nlu_ru\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m#Tokenize sentences\u001B[39;00m\n\u001B[0;32m     17\u001B[0m encoded_input \u001B[38;5;241m=\u001B[39m tokenizer(sentences, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m24\u001B[39m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:566\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    565\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 566\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    567\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    568\u001B[0m     )\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    572\u001B[0m )\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\transformers\\modeling_utils.py:3236\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   3233\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001B[38;5;241m=\u001B[39mtorch_dtype, device_map\u001B[38;5;241m=\u001B[39mdevice_map)\n\u001B[0;32m   3235\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ContextManagers(init_contexts):\n\u001B[1;32m-> 3236\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(config, \u001B[38;5;241m*\u001B[39mmodel_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[0;32m   3238\u001B[0m \u001B[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001B[39;00m\n\u001B[0;32m   3239\u001B[0m config \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:883\u001B[0m, in \u001B[0;36mBertModel.__init__\u001B[1;34m(self, config, add_pooling_layer)\u001B[0m\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n\u001B[0;32m    882\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings \u001B[38;5;241m=\u001B[39m BertEmbeddings(config)\n\u001B[1;32m--> 883\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m \u001B[43mBertEncoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    885\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;241m=\u001B[39m BertPooler(config) \u001B[38;5;28;01mif\u001B[39;00m add_pooling_layer \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    887\u001B[0m \u001B[38;5;66;03m# Initialize weights and apply final processing\u001B[39;00m\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:560\u001B[0m, in \u001B[0;36mBertEncoder.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m--> 560\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList([BertLayer(config) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mnum_hidden_layers)])\n\u001B[0;32m    561\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_checkpointing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:560\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m--> 560\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList([\u001B[43mBertLayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mnum_hidden_layers)])\n\u001B[0;32m    561\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_checkpointing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:475\u001B[0m, in \u001B[0;36mBertLayer.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_size_feed_forward \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mchunk_size_feed_forward\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq_len_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 475\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention \u001B[38;5;241m=\u001B[39m \u001B[43mBertAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_decoder \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mis_decoder\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_cross_attention \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39madd_cross_attention\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:395\u001B[0m, in \u001B[0;36mBertAttention.__init__\u001B[1;34m(self, config, position_embedding_type)\u001B[0m\n\u001B[0;32m    393\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config, position_embedding_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    394\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m--> 395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself \u001B[38;5;241m=\u001B[39m \u001B[43mBertSelfAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_embedding_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embedding_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    396\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m BertSelfOutput(config)\n\u001B[0;32m    397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpruned_heads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:257\u001B[0m, in \u001B[0;36mBertSelfAttention.__init__\u001B[1;34m(self, config, position_embedding_type)\u001B[0m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_head_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(config\u001B[38;5;241m.\u001B[39mhidden_size \u001B[38;5;241m/\u001B[39m config\u001B[38;5;241m.\u001B[39mnum_attention_heads)\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_head_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_attention_heads \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_head_size\n\u001B[1;32m--> 257\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_head_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(config\u001B[38;5;241m.\u001B[39mhidden_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_head_size)\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(config\u001B[38;5;241m.\u001B[39mhidden_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_head_size)\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\torch\\nn\\modules\\linear.py:101\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[1;34m(self, in_features, out_features, bias, device, dtype)\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_parameter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbias\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 101\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\torch\\nn\\modules\\linear.py:109\u001B[0m, in \u001B[0;36mLinear.reset_parameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    107\u001B[0m init\u001B[38;5;241m.\u001B[39mkaiming_uniform_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, a\u001B[38;5;241m=\u001B[39mmath\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;241m5\u001B[39m))\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 109\u001B[0m     fan_in, _ \u001B[38;5;241m=\u001B[39m \u001B[43minit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_calculate_fan_in_and_fan_out\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m     bound \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(fan_in) \u001B[38;5;28;01mif\u001B[39;00m fan_in \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    111\u001B[0m     init\u001B[38;5;241m.\u001B[39muniform_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;241m-\u001B[39mbound, bound)\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\torch\\nn\\init.py:284\u001B[0m, in \u001B[0;36m_calculate_fan_in_and_fan_out\u001B[1;34m(tensor)\u001B[0m\n\u001B[0;32m    279\u001B[0m                     tensor[g \u001B[38;5;241m*\u001B[39m out_chans_per_grp \u001B[38;5;241m+\u001B[39m d, d, tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m    280\u001B[0m                            tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m3\u001B[39m) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m, tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m4\u001B[39m) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tensor\n\u001B[1;32m--> 284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_calculate_fan_in_and_fan_out\u001B[39m(tensor):\n\u001B[0;32m    285\u001B[0m     dimensions \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39mdim()\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dimensions \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "sentences = ['Привет! Как твои дела?',\n",
    "             'А правда, что 42 твое любимое число?']\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_mt_nlu_ru\")\n",
    "model = AutoModel.from_pretrained(\"ai-forever/sbert_large_mt_nlu_ru\")\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:11.350189700Z",
     "start_time": "2023-11-05T16:30:52.247690Z"
    }
   },
   "id": "3122a81a5ee37d73"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1024])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:53:58.432152Z",
     "start_time": "2023-11-05T15:53:58.403500600Z"
    }
   },
   "id": "fb01e6a52685f280"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "encoded_input = tokenizer([\"Системный администратор ббебебебе с бабабабаба\"], padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:57:45.745097300Z",
     "start_time": "2023-11-05T15:57:45.614121Z"
    }
   },
   "id": "8bedfc825ae34c6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a9872fac276e8c27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
