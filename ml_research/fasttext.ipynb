{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:49:32.623782800Z",
     "start_time": "2023-11-05T16:49:11.517196200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "df = pd.read_csv('resume_data.csv')\n",
    "df.drop_duplicates(inplace=True)\n",
    "stop_words = set(stopwords.words('russian'))  # Используйте соответствующий язык\n",
    "stemmer = SnowballStemmer('russian')  # Используйте соответствующий языкэ\n",
    "def preprocess_text(text):\n",
    "    # Приведем к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удалим пунктуацию\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    # Токенизация\n",
    "    words = word_tokenize(text)\n",
    "    # Удалим стоп-слова и применим стемминг\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "job_description = \"\"\"\n",
    "Системный администратор\n",
    "Знание и опыт работы с серверными операционными системами Windows Server и Linux.\n",
    "Умение настраивать сетевое оборудование.\n",
    "Опыт работы с системами виртуализации и облачными сервисами.\n",
    "Способность быстро решать проблемы с IT-инфраструктурой.\n",
    "Навыки работы с базами данных и системами резервного копирования.\n",
    "\"\"\"\n",
    "\n",
    "text_columns = ['Position', 'Specializations', 'Previous_Positions', 'Languages', 'Education', 'About_Me', 'Skills']\n",
    "resume_texts = df[text_columns].fillna('').agg(' '.join, axis=1).tolist()\n",
    "all_texts = [job_description] + resume_texts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:49:32.655786200Z",
     "start_time": "2023-11-05T16:49:32.616784Z"
    }
   },
   "id": "be71fcf059da2351"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpairwise\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cosine_similarity\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_facebook_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcc.ru.300.bin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\gensim\\models\\fasttext.py:728\u001B[0m, in \u001B[0;36mload_facebook_model\u001B[1;34m(path, encoding)\u001B[0m\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_facebook_model\u001B[39m(path, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    667\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load the model from Facebook's native fasttext `.bin` output file.\u001B[39;00m\n\u001B[0;32m    668\u001B[0m \n\u001B[0;32m    669\u001B[0m \u001B[38;5;124;03m    Notes\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    726\u001B[0m \n\u001B[0;32m    727\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 728\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_fasttext_format\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\gensim\\models\\fasttext.py:840\u001B[0m, in \u001B[0;36m_load_fasttext_format\u001B[1;34m(model_file, encoding, full_model)\u001B[0m\n\u001B[0;32m    826\u001B[0m model\u001B[38;5;241m.\u001B[39mvocab_size \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39mvocab_size\n\u001B[0;32m    828\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    829\u001B[0m \u001B[38;5;66;03m# This is here to fix https://github.com/RaRe-Technologies/gensim/pull/2373.\u001B[39;00m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;66;03m# trimmed raw_vocab, so this change does not affect them.\u001B[39;00m\n\u001B[0;32m    839\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m--> 840\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mupdate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    842\u001B[0m model\u001B[38;5;241m.\u001B[39mnum_original_vectors \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39mvectors_ngrams\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    844\u001B[0m model\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39minit_post_load(m\u001B[38;5;241m.\u001B[39mvectors_ngrams)\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\gensim\\models\\word2vec.py:742\u001B[0m, in \u001B[0;36mWord2Vec.prepare_vocab\u001B[1;34m(self, update, keep_raw_vocab, trim_rule, min_count, sample, dry_run)\u001B[0m\n\u001B[0;32m    740\u001B[0m         downsample_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m v\n\u001B[0;32m    741\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dry_run:\n\u001B[1;32m--> 742\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_vecattr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msample_int\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muint32\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword_probability\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dry_run \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m keep_raw_vocab:\n\u001B[0;32m    745\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeleting the raw counts dictionary of \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m items\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw_vocab))\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\gensim\\models\\keyedvectors.py:334\u001B[0m, in \u001B[0;36mKeyedVectors.set_vecattr\u001B[1;34m(self, key, attr, val)\u001B[0m\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpandos[attr] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(target_size, dtype\u001B[38;5;241m=\u001B[39mprev_expando\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpandos[attr][: \u001B[38;5;28mmin\u001B[39m(prev_count, target_size), ] \u001B[38;5;241m=\u001B[39m prev_expando[: \u001B[38;5;28mmin\u001B[39m(prev_count, target_size), ]\n\u001B[1;32m--> 334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mset_vecattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, attr, val):\n\u001B[0;32m    335\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Set attribute associated with the given key to value.\u001B[39;00m\n\u001B[0;32m    336\u001B[0m \n\u001B[0;32m    337\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    351\u001B[0m \n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mallocate_vecattrs(attrs\u001B[38;5;241m=\u001B[39m[attr], types\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mtype\u001B[39m(val)])\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = load_facebook_model('cc.ru.300.bin')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:01:22.188654600Z",
     "start_time": "2023-11-05T16:59:24.011636800Z"
    }
   },
   "id": "cc4c50cb0dbb34dc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xba in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Предполагается, что у вас есть предобученная модель FastText в файле 'fasttext.model.bin'\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Загружаем предобученную модель FastText\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mKeyedVectors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcc.ru.300.bin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Функция для получения вектора документа путем усреднения векторов слов\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdocument_vector\u001B[39m(doc):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# Удаляем слова, которых нет в модели\u001B[39;00m\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001B[0m, in \u001B[0;36mKeyedVectors.load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[0;32m   1672\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m   1673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_word2vec_format\u001B[39m(\n\u001B[0;32m   1674\u001B[0m         \u001B[38;5;28mcls\u001B[39m, fname, fvocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m'\u001B[39m, unicode_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1675\u001B[0m         limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, datatype\u001B[38;5;241m=\u001B[39mREAL, no_header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1676\u001B[0m     ):\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001B[39;00m\n\u001B[0;32m   1678\u001B[0m \n\u001B[0;32m   1679\u001B[0m \u001B[38;5;124;03m    Warnings\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1717\u001B[0m \n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1720\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1721\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1722\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2058\u001B[0m, in \u001B[0;36m_load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[0;32m   2056\u001B[0m     fin \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mopen(fname, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   2057\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2058\u001B[0m     header \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_unicode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2059\u001B[0m     vocab_size, vector_size \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mint\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m header\u001B[38;5;241m.\u001B[39msplit()]  \u001B[38;5;66;03m# throws for invalid file format\u001B[39;00m\n\u001B[0;32m   2060\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m limit:\n",
      "File \u001B[1;32mD:\\All_python_trash\\lib\\site-packages\\gensim\\utils.py:365\u001B[0m, in \u001B[0;36many2unicode\u001B[1;34m(text, encoding, errors)\u001B[0m\n\u001B[0;32m    363\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n\u001B[1;32m--> 365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xba in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "\n",
    "# Функция для получения вектора документа путем усреднения векторов слов\n",
    "def document_vector(doc):\n",
    "    # Удаляем слова, которых нет в модели\n",
    "    words = [word for word in doc.split() if word in model.vocab]\n",
    "    if len(words) == 0:\n",
    "       return np.zeros(model.vector_size)\n",
    "    else:\n",
    "       # Усредняем векторы слов, чтобы получить вектор документа\n",
    "       return np.mean(model[words], axis=0)\n",
    "\n",
    "# Преобразуем описание вакансии и резюме в векторы\n",
    "job_vec = document_vector(job_description)\n",
    "resume_vecs = np.array([document_vector(doc) for doc in resume_texts])\n",
    "\n",
    "# Вычисляем косинусное сходство между вакансией и каждым резюме\n",
    "cosine_similarities = cosine_similarity([job_vec], resume_vecs).flatten()\n",
    "\n",
    "# Создаем словарь сопоставлений индекс резюме -> сходство\n",
    "resume_similarities = dict(enumerate(cosine_similarities))\n",
    "\n",
    "# Сортируем резюме по убыванию сходства\n",
    "sorted_resume_similarities = sorted(resume_similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "sorted_resume_similarities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:53:59.991615500Z"
    }
   },
   "id": "917750a6d690fbc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d7b242b07e122a4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
